\documentclass[12pt,addpoints]{exam}

\newcommand{\class}{10-423/10-623 Gen AI}
\newcommand{\term}{Fall 2024}
\newcommand{\examnum}{Quiz 2}
\newcommand{\examdate}{09/30/24}
\newcommand{\timelimit}{15 minutes} % This one was 18-20 minutes in S24
%% To HIDE SOLUTIONS, set this value to 0: 
% \providecommand{\issoln}{0}
\providecommand{\issoln}{1}

\input{../shared_quiz/configurations.tex}

\begin{document}

\input{../shared_quiz/instructions.tex}

% \input{../shared/instructions_for_specific_problem_types.tex}
% \clearpage

\begin{questions}

\sectionquestion{Deep Models for Vision}
\begin{parts}

\part[2] \textbf{Select all that apply:} Suppose you are training a CNN and you are concerned that the dimensionality of your first convolutional layer's output is too large. Which of the following changes to your model's architecture would cause the first convolutional layer's output dimensionality to \emph{decrease}?
\begin{checkboxessquare}
     \choice Increasing the stride in this layer
     \choice Increasing the padding around the input to this layer
     \choice Increasing the dimensionality of the input to this layer
     \choice Increasing the filter size in this layer
     \choice None of the above
\end{checkboxessquare}
\begin{soln}
    A, D
\end{soln}
\begin{qauthor} 
    Henry
\end{qauthor}

\part[1] \textbf{True or False:} The primary difference between a decoder-only transformer and an encoder-only transformer is that a token in a decoder-only transformer cannot attend to tokens that come \emph{after} it in the input sequence whereas a token in an encoder-only transformer cannot attend to tokens that come \emph{before} it in the input sequence. 
\begin{checkboxes}
    \choice True 
    \choice False
\end{checkboxes}
\begin{soln}
    False
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\begin{comment}
\part[1] \textbf{Select one:} What component of an image does a vision transformer use as the \emph{tokens} when constructing an input sequence? 
\begin{checkboxes}
    \choice RGB channels
    \choice Relevant macro-features 
    \choice Fixed-size patches
    \choice Individual pixels
\end{checkboxes}
\begin{soln}
    C
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}
\end{comment}

\end{parts}

\clearpage
\sectionquestion{GANs}
\begin{parts}

\part[1] \textbf{True or False:} In a generative adversarial network, both the discriminator and the generator must be convolutional neural networks. 
\begin{checkboxes}
    \choice True 
    \choice False
\end{checkboxes}
\begin{soln}
    False
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\part[1] \textbf{Select one:} Which of the following statements best describes how the hyperparameter $k$ is used in the training of a GAN?
\begin{checkboxes}
    \choice The discriminator is trained to convergence $k$ times, each time using a different random initialization. Then the discriminator with the lowest loss is used to train the generator to convergence. 
    \choice The generator is trained to convergence $k$ times, each time using a different random initialization. Then the generator with the lowest loss is used to train the discriminator to convergence. 
    \choice The discriminator \emph{and} the generator are trained simultaneously for $k$ steps of mini-batch SGD. Then, keeping the discriminator fixed, the generator is trained for one step of mini-batch SGD. 
    \choice The discriminator is trained for $k$ steps of mini-batch SGD while keeping the generator fixed. Then, keeping the discriminator fixed, the generator is trained for one step of mini-batch SGD. 
\end{checkboxes}
\begin{soln}
    D
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}
\end{parts}

\clearpage
\sectionquestion{VAEs}
\begin{parts}

\part[2] \textbf{Select all that apply:} Which of the following statements about the KL divergence term in the VAE objective function \emph{as presented in Lecture 6} is/are true?
\begin{checkboxessquare}
     \choice The arguments of the KL divergence are the decoder's learned distribution over images and a prior distribution over images in the training dataset. 
     \choice The KL divergence term is meant to encourage the encoder to learn a dense distribution over the latent space.
     \choice A common choice for the prior distribution is a uniform distribution over the unit hypercube.
     \choice The KL divergence term acts as a regularizer and can prevent the model from overfitting to the empirical distribution of images in the training dataset.
     \choice None of the above
\end{checkboxessquare}
\begin{soln}
    B, D
\end{soln}
\begin{qauthor} 
    Henry
\end{qauthor}

\part[1] \textbf{Select one:} Which equation below represents how a VAE uses the \emph{reparameterization trick} to model the random variable $\mathbf{z} \sim \mathcal{N}(\boldsymbol\mu_{\boldsymbol\theta}, \boldsymbol\sigma^2_{\boldsymbol\theta})$?
\begin{checkboxes}
    \choice $z = \boldsymbol\mu_{\boldsymbol\theta} + \boldsymbol\epsilon$ where $\boldsymbol\epsilon \sim \mathcal{N}(\mathbf{0}, \boldsymbol\sigma^2_{\boldsymbol\theta})$
    \choice $z = \boldsymbol\sigma_{\boldsymbol\theta}^T\boldsymbol\epsilon$ where $\boldsymbol\epsilon \sim \mathcal{N}(\boldsymbol\mu_{\boldsymbol\theta}, I)$
    \choice $z = \boldsymbol\sigma_{\boldsymbol\theta} \odot \boldsymbol\epsilon$ where $\boldsymbol\epsilon \sim \mathcal{N}(\boldsymbol\mu_{\boldsymbol\theta}, I)$
    \choice $z = \boldsymbol\mu_{\boldsymbol\theta} + \boldsymbol\sigma_{\boldsymbol\theta}^T\boldsymbol\epsilon$ where $\boldsymbol\epsilon \sim \mathcal{N}(\mathbf{0}, I)$
    \choice $z = \boldsymbol\mu_{\boldsymbol\theta} + \boldsymbol\sigma_{\boldsymbol\theta} \odot \boldsymbol\epsilon$ where $\boldsymbol\epsilon \sim \mathcal{N}(\mathbf{0}, I)$
\end{checkboxes}
\begin{soln}
    E
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\part[1] \textbf{True or False:} In a VAE, the variational approximation is also called a ``decoder'' and gives a distribution over images $\xv$ conditioned on a latent $\zv$. 
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    Input solution here.
    \end{soln}
    \begin{qauthor}
    False, the decoder $p_{\phi}(\xv \mid \zv)$ is part of the model $p_{\phi}(\xv, \zv)$ we wish to approximate, the encoder $q_{\theta}(\zv \mid \xv)$ is the variational approximation to the model's posterior $p_{\phi}(\zv \mid \xv)$. 
    \end{qauthor}

\end{parts}

\clearpage
\sectionquestion{Diffusion Models}
\begin{parts}

\part[2] \textbf{Select all that apply:} Which of the following are valid statements about the \emph{exact} reverse process $q_{\phi}(\xv_T) \prod_{t=1}^T q_{\phi}(\xv_{t-1} \mid \xv_t)$ in a Denoising Diffusion Probabilistic Model (DDPM)?
    \begin{checkboxessquare}
     \choice $q_{\phi}(\xv_{t-1} \mid \xv_t)$ can be computed efficiently in closed form because it is a product of Gaussian densities
     \choice $q_{\phi}(\xv_{t-1} \mid \xv_t)$ cannot be computed efficiently because of its dependence on $q(\xv_0)$
     \choice The parameters $\phi$ of the exact reverse process are learned from unlabeled training images
     \choice The parameters $\phi$ of the exact reverse process are fully determined by $\alpha_1, \ldots, \alpha_T$
     \choice The parameters $\phi$ of the exact reverse process are the mean/variance of a series of Gaussian distributions
     \choice None of the above
    \end{checkboxessquare}
    \begin{soln}
    B, D, E
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}


\part[1] \textbf{True or False:} If we have all the parameters of a DDPM and are given an input image $\xv_0$, then we can efficiently sample from the true distribution over latent states $\xv_t$ without resorting to iteration.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True, $\xv_t = \sqrt{\bar{\alpha}_t} \xv_{0} + (1-\bar{\alpha}_t) \epsilonv$ where $\epsilonv \sim \Nc(\mathbf{0}, \Iv)$
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}

% \part[1] \textbf{True or False:} If we have all the parameters of a DDPM and are given a latent state $\xv_t$, then we can efficiently sample from the true distribution over input images $\xv_0$.
%     \begin{checkboxes}
%      \choice True 
%      \choice False
%     \end{checkboxes}
%     \begin{soln}
%     False, we cannot sample from the true distribution over input images $q(\xv_0)$, we can only sample from the model's distribution over input images.
%     \end{soln}
%     \begin{qauthor}
%     Matt
%     \end{qauthor}


\part[1] \textbf{True or False:} Regardless of how the UNet is used to parameterize a DDPM, the goal of training is to learn parameters in order to subtract away noise from a known training image $\xv_0$ to which we added noise.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}

\end{parts}

\end{questions}

% Commented for quizzes
%\input{../../shared/scratch_pages.tex}

\end{document}