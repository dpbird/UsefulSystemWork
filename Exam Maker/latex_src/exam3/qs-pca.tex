\sectionquestion{Principal Component Analysis}

\begin{parts}

\part[2] \textbf{Select all that apply:} Why \textbf{might} performing dimensionality reduction using PCA be bad for a classification task?
    \begin{checkboxessquare}
        \choice  It removes variability without knowledge of the variables that might be most useful in classification.
        \choice  It transforms a number of possibly correlated
    coordinates into a smaller number of dimensions.
        \choice  The data cannot be well approximated by a linear subspace.        
        \choice  It removes redundancy in the input representation.
        \choice None of the above
    \end{checkboxessquare}    
    \begin{soln}
    (a) It removes variability without knowledge of the variables that might be most useful in classification.\\
    (c) The data cannot be well approximated by a linear subspace.
    \end{soln}
    \begin{qauthor}
        Matt (adapted from S17)
    \end{qauthor}

\begingroup\setlength{\baselineskip}{1.7\baselineskip}
\part[2] \textbf{Fill in the blanks:} PCA can be interpreted as optimizing two equivalent objective functions. Under one interpretation, it maximizes the \\ \underline{\hspace{18em}} of the projection along each component. Under the other it minimizes the  \underline{\hspace{18em}}.
\par\endgroup
    \begin{soln}
    variance, reconstruction error.
    \end{soln}
    \begin{qauthor}
        Matt (adapted from S17)
    \end{qauthor}


\part[1] \textbf{True or False:} When projecting from $M$ dimensions down to $K$ dimensions with the random projection algorithm (i.e. $\uv^{(i)} = \Vv \xv^{(i)}$ where $V_{km} \sim \text{Gaussian}(0,1)$), the distance between any two points in the $K$ dimensional space will be zero with high probability, regardless of their distance in $M$ dimensional space.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False. 
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}

\part[1] \textbf{True or False:} In PCA, the eigenvalue of the first principal component equals the amount of variability captured along the axis defined by that principal component.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True.
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}

    
\part[1] \textbf{True or False:} Singular value decomposition is an algorithm for finding the principal components for PCA that iteratively finds the first principal component, then the second, then the third, and so on.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False. SVD finds all the principal components at once.
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}


\end{parts}