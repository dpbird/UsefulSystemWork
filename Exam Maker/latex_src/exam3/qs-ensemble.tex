\sectionquestion{Ensemble Methods}

\begin{parts}
    \part Professor Oak trains a random forest consisting of 3 decision trees, $\{t_1, t_2, t_3\}$, on a binary classification dataset with 5 data points. He sends Professor Elm the following table, that summarizes which data points were used to train each decision tree as well as what each decision tree would predict on the individual data points: 
    \begin{center}
        \begin{tabular}{c c|c c c|c c c} 
            & & Used to & Used to & Used to & & & \\
            $i$ & Label & train $t_1$ & train $t_2$ & train $t_3$ & $t_1(x^{(i)})$ & $t_2(x^{(i)})$ & $t_3(x^{(i)})$ \\ \hline
            1 & + & \checkmark & \checkmark & & + & - & + \\ 
            2 & - & & \checkmark & & - & - & - \\ 
            3 & - & \checkmark & & \checkmark & - & + & - \\ 
            4 & + & \checkmark & & & - & + & + \\ 
            5 & + & & & \checkmark & - & - & + \\ 
            \hline
        \end{tabular}
    \end{center}

    \begin{subparts}
        \subpart[1] \textbf{Numerical answer:} What is the training error rate of Professor Oak's random forest? Break ties in majority votes in favor of the + label. Express your answer as a simplified fraction. 
        \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
            \begin{soln}
                $\frac{1}{5}$
            \end{soln}
        \end{tcolorbox}
        
        \subpart[1] \textbf{Numerical answer:} What is the out-of-bag error rate of Professor Oak's random forest? Break ties in majority votes in favor of the + label. Express your answer as a simplified fraction. 
        \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
            \begin{soln}
                $\frac{2}{5}$
            \end{soln}
        \end{tcolorbox}
    \end{subparts}
    
    \begin{qauthor}
        Henry
    \end{qauthor}
    
    \part[2] \textbf{Select all that apply:} Suppose you run the weighted majority algorithm with two linear decision boundaries, $h_1$ and $h_2$, as your initial classifiers and you learn classifier weights $\alpha_1$ and $\alpha_2$. Assuming sign$(0)=+1$, which of the following conditions \emph{could} result in the learned ensemble having a non-linear decision boundary? 
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice $h_1$ and $h_2$ are not parallel \emph{and} $\alpha_1 \ne \alpha_2$
     \choice $h_1$ and $h_2$ are not parallel \emph{and} $\alpha_1 = \alpha_2$
     \choice $h_1$ and $h_2$ are parallel \emph{and} $\alpha_1 \ne \alpha_2$
     \choice $h_1$ and $h_2$ are parallel \emph{and} $\alpha_1 = \alpha_2$
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
        B, D
    \end{soln}
    \begin{qauthor}
        Henry
    \end{qauthor}

\clearpage
    \part[2] \textbf{Drawing:} The figure below shows a set of linear decision boundaries and the corresponding weights learned using the weighted majority algorithm; the colored arrow matching each decision boundary corresponds to the side that each classifier predicts as positive. 

    \begin{center}
        \begin{tikzpicture}
        \begin{axis}[
            scale=1.3, axis equal image, mark options={scale=1.5},
            xmin=0, xmax=6, xtick={}, xticklabel=\empty,
            ymin=0, ymax=6, ytick={}, yticklabel=\empty,
            samples=50]
            \addplot[red, ultra thick] coordinates { (4, 0) (4, 6) };
            \node[red] at (4.75, 5.5) {$\alpha_1 = 0.2$};
            \addplot[blue, ultra thick] coordinates { (6, 5) (0, 2) };
            \node[blue] at (0.75, 1.75) {$\alpha_2 = 0.8$};
            \addplot[black, ultra thick] coordinates { (5, 0) (0, 5) };
            \node[black] at (0.75, 5) {$\alpha_3 = 0.9$};
            \draw [-{Latex[length=3mm]}, red, ultra thick] (4,3) -- (3,3);
            \draw [-{Latex[length=3mm]}, blue, ultra thick] (3,3.5) -- (2.5,4.5);
            \draw [-{Latex[length=3mm]}, black, ultra thick] (3,2) -- (2.25,1.25);
        \end{axis}
        \end{tikzpicture} 
    \end{center}

    On the figure, lightly shade the area(s) that would be classified as positive by the ensemble formed using these decision boundaries and weights. 
    
    \begin{soln}
        \begin{center}
            \begin{tikzpicture}
            \begin{axis}[
                scale=1.3, axis equal image, mark options={scale=1.5},
                xmin=0, xmax=6, xtick={}, xticklabel=\empty,
                ymin=0, ymax=6, ytick={}, yticklabel=\empty,
                samples=50]
                \addplot[red, ultra thick] coordinates { (4, 0) (4, 6) };
                \node[red] at (4.75, 5.5) {$\alpha_1 = 0.2$};
                \addplot[blue, ultra thick] coordinates { (6, 5) (0, 2) };
                \node[blue] at (0.75, 1.75) {$\alpha_2 = 0.8$};
                \addplot[black, ultra thick] coordinates { (5, 0) (0, 5) };
                \node[black] at (0.75, 5) {$\alpha_3 = 0.9$};
                \draw [-{Latex[length=3mm]}, red, ultra thick] (4,3) -- (3,3);
                \draw [-{Latex[length=3mm]}, blue, ultra thick] (3,3.5) -- (2.5,4.5);
                \draw [-{Latex[length=3mm]}, black, ultra thick] (3,2) -- (2.25,1.25);
                \draw [fill=gray!30, fill opacity=0.3] (0, 0) -- (0, 6) -- (4, 6) -- (4, 4) -- (2, 3) -- (4, 1) -- (4, 0) -- cycle;
            \end{axis}
            \end{tikzpicture} 
        \end{center}
    \end{soln}
    

    \part[1] \textbf{Numerical answer:} While running AdaBoost, Neural observes that the weights on all of his data points do not change from iteration $t$ to iteration $t+1$, i.e., $D_{t+1}(i) = D_t(i)$ $\forall$ $i$, regardless of whether or not they were multiplied by $\exp(-\alpha_t)$ or $\exp(\alpha_t)$ where $\alpha_t = \frac{1}{2}\ln \frac{1-\epsilon_t}{\epsilon_t}$. What was $\epsilon_t$, the weighted training error of the weak learner in the $t^{\textrm{th}}$ iteration?
    
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
        \begin{soln}
            $\frac{1}{2}$
        \end{soln}
    \end{tcolorbox}
    \begin{qauthor}
        Henry
    \end{qauthor}
\end{parts}