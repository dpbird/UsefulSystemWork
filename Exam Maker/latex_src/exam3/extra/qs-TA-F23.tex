\sectionquestion{F23 TA Questions Go Here!} 

\begin{parts}


\part[2] \textbf{Short answer:} Suppose we have n data points $\{x_i,y_i\}_{i=1}^n$, $x_i, y_i \in \mathbb{R}$. Assume that these points were sampled from some distribution $\mathcal{P}$, which is unknown. You decide to fit a linear regression model as $\hat{y} = w_1 x + w_0$. However, since these points were sampled at random, as the sample changes, the weights obtained by training a linear regression model on the sample will change as well. We are interested in calculating the variance in the parameters $w_1,w_0$, as we get different samples. Provide an algorithm to do so using bootstrapping.
    \fillwithlines{10em}
    \begin{soln}
    For some iterations $t$, sample $n$ points from the initial set of points with replacement using bootstrapping, compute $w_1, w_0$ using these points for each $t$ and then take the variance across the $t$ iterations. 
    \end{soln}
    \begin{qauthor}
    Pranit Bootstrapping
    \end{qauthor}

\part Each of the following descriptions describes one step of an optimization algorithm. Select the appropriate algorithm name, or indicate that none apply.
\begin{subparts}
    \subpart[1] \textbf{Select one:} Using one dimension of the training data (or ``coordinate''), optimize all parameters.
    \begin{checkboxes}
     \choice Coordinate descent
     \choice Block coordinate descent
     \choice Neither
    \end{checkboxes}
    \begin{soln}
    Neither.
    \end{soln}

    \subpart[1] \textbf{Select one:} Fix all but one parameter (or ``coordinate''), and optimize the remaining parameter.
    \begin{checkboxes}
     \choice Coordinate descent
     \choice Block coordinate descent
     \choice Neither
    \end{checkboxes}
    \begin{soln}
    Coordinate descent
    \end{soln}

    \subpart[1] \textbf{Select one:} Using one subset of dimensions of the training data (or ``block of coordinates''), optimize all parameters.
    \begin{checkboxes}
     \choice Coordinate descent
     \choice Block coordinate descent
     \choice Neither
    \end{checkboxes}
    \begin{soln}
    Neither.
    \end{soln}

    \subpart[1] \textbf{Select one:} Fix all but one set of parameters (or ``block of coordinates''), and optimize the remaining parameters.
    \begin{checkboxes}
     \choice Coordinate descent
     \choice Block coordinate descent
     \choice Neither
    \end{checkboxes}
    \begin{soln}
    Block coordinate descent
    \end{soln}
\end{subparts}
    \begin{qauthor}
    Abhi. K-Means

    Distinguish between coordinate descent and block coordinate descent
    \end{qauthor}

 


\part[1] Which of the following are key assumptions of PCA?
{\checkboxchar{$\Box$} \checkedchar{$\blacksquare$}
\begin{checkboxes}
    \choice The data exists in a low k-dimensional subspace 
    \choice The features of the data are not correlated or have low correlation
    \choice The features have a linear relationship
    \choice The data is standardized
    \choice None of the above   
\end{checkboxes}}

\begin{soln}
    A,C,D
\end{soln}
\begin{qauthor}
    Tara, PCA
\end{qauthor}

\begin{qtester}
Note that some presentations of standard PCA require mean-centered data but not standardized data.
\end{qtester}


\part Recall that K-Means searches for $k$ centroids $c_z; z\in \{1,\dots ,k\}$ for $n$ data points $\{x_1, \dots , x_n \}$ which minimize the Euclidean distance for the data points to the centroids. 
\begin{subparts}
    

    \subpart[2]What is the objective function of K-Means?\\
    You can denote the Euclidean distance between two points $a$ and $b$ as $\|a-b\|_2^2$
    

    \begin{tcolorbox}[fit,height=3cm, width=15cm, blank, borderline={1pt}{-2pt}]
    \begin{soln}
    $$J(c,z) = \sum_{i=1}^n  \|x^{(i)} - c_z\|_2$$
    \end{soln}
    \end{tcolorbox}


\subpart[2] Why is random initialization not guaranteed to converge to the optimal clustering.

\fillwithlines{7em}

\begin{soln}
    The objective function of K-Means is non-convex. The random initialization method can result in the algorithm getting stuck at a local minima if two or more centroids are initialized in the same optimal cluster.
\end{soln}
\begin{qauthor}
        Tara, K-Means Define an objective function of K-Means which gives rise to a good clustering
\end{qauthor}

\begin{qtester}
Euclidean distance should be replaced with squared Euclidean distance in both places it appears in the question, right?

Part (b) seems potentially too open-ended, and you might get better answers by not asking them to try to relate it to the objective function written above.
\end{qtester}

\end{subparts}

\part[2] \textbf{True or False:} If we use the random subspace method with $k=\frac{1}{2}d$ where $d$ is the dimension of the data, then the time complexity of classification \textit{(not the big-O)} for KNN is half of the original time complexity.

\begin{checkboxes}
    \choice True
    \choice False
\end{checkboxes}

\fillwithlines{5em}
\begin{soln}
    True, the original time complexity of KNN is $nd$, with the Random subspace method 
\end{soln}

\begin{qtester}
    I'm not sure if we can, but if possible we might be able to reword/formalize this. We should also potentially state we want the naive algorithm for KNN, as we presented 2 in class.
\end{qtester}

\part Consider an HMM with two states $S_1$ and $S_2$ and two possible observations $O_1$ and $O_2$. The state transition matrix is given by
$$A = \begin{pmatrix}
0.6 & 0.4 \\
0.3 & 0.7
\end{pmatrix}$$ and the observation matrix is given by
$$B = \begin{pmatrix}
0.2 & 0.8 \\
0.9 & 0.1
\end{pmatrix}$$ You may assume that the initial state probabilities are uniform, i.e., $\pi(S_1)=\pi(S_2)=0.5$.

    \begin{subparts}
    \subpart[1] Run the forward algorithm to compute the  probabilities $\alpha_1(S_1), \alpha_1(S_2)$, $\alpha_2(S_1), \alpha_2(S_2)$ given the observation sequence $O=O_1 O_2$.
    \addpoints
    \subpart[1] Assume the backward algorithm is similarly already run for you. Can you explain how the above forward-backward algorithm can be interpreted as a message-passing algorithm? Show the message passing equations (in terms of variables) for computing the messages. 
    \addpoints
    \end{subparts}
    \begin{soln}
        \begin{enumerate}
            \item Using the backward algorithm, we can compute the marginal probabilities $P(S_t|O)$ for $t=1$ and $t=2$ as follows:
            \begin{itemize}
                \item Forward algorithm:
                    \begin{align*}
                    \alpha_1(S_1) &= b_{11} \cdot \pi_{S_1} = 0.2 \cdot 0.5 = 0.1\\
                    \alpha_1(S_2) &= b_{21} \cdot \pi_{S_2} = 0.9 \cdot 0.5 = 0.45\\
                    \alpha_2(S_1) &= \sum_{i=1}^2 \alpha_1(S_i) \cdot a_{i1} \cdot b_{1O_2} = (0.1 \cdot 0.6 \cdot 0.8) + (0.45 \cdot 0.3 \cdot 0.1) = 0.042\\
                    \alpha_2(S_2) &= \sum_{i=1}^2 \alpha_1(S_i) \cdot a_{i2} \cdot b_{2O_2} = (0.1 \cdot 0.4 \cdot 0.1) + (0.45 \cdot 0.7 \cdot 0.1) = 0.075
                    \end{align*}
            \end{itemize}
            \item The message-passing equations for the forward-backward algorithm can be written as follows:
            \begin{itemize}
                \item Forward Messages:
                \begin{align}
                \alpha_t(j) &= P(O_1, O_2, \ldots, O_t, q_t = S_j | \lambda) \
                &= \sum_{i=1}^2 \alpha_{t-1}(i) \cdot a_{ij} \cdot b_{j}(O_t)
                \end{align}
                \item Backward Messages:
                \begin{align}
                \beta_t(i) &= P(O_{t+1}, O_{t+2}, \ldots, O_T | q_t = S_i, \lambda) \
                &= \sum_{j=1}^2 a_{ij} \cdot b_j(O_{t+1}) \cdot \beta_{t+1}(j)
                \end{align}
            \end{itemize}
            \item 
        \end{enumerate}
    \end{soln}
    \begin{qauthor}
        Yash, HMM
    \end{qauthor}

\end{parts}
