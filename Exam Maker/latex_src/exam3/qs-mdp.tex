\sectionquestion{MDPs and Value/Policy Iteration}

\begin{parts}


\part Suppose we wish to learn to play the classic side-scrolling computer game Lemmings. For the first level, each state $s=(s_{ij}, s_d)$ in state space $\Sc$ contains the lemming's position ($s_{ij}$), the lemming's direction ($s_d \in \{\text{left}, \text{right}\}$). The action space is $\Ac = \{$ \texttt{continue}, \texttt{dig} $\}$. 
\begin{itemize}
    \item The \texttt{continue} action allows the lemming to walk one tile in its current direction (left or right). 
    \\
    \textit{Example:} From  $s=(s_{22}, \text{right})$ taking \texttt{continue} yields $s=(s_{23}, \text{right})$.
    
    \item Physics applies: if the direction $s_d$ leads into solid tile (brown or gray), the lemming position remains unchanged but its direction switches. 
    \\
    \textit{Example:} From $s=(s_{47}, \text{right})$ taking \texttt{continue} yields $s=(s_{47}, \text{left})$.
    
    \item Gravity applies: If the lemming is above a non-solid tile (white or blue), it will fall down one tile regardless of its direction. %\textit{Example:} if the lemming is moving right from $s_{55}$, in one turn it moves to $s_{56}$, and in two turns to $s_{46}$. 
    \\
    \textit{Example:} From $s=(s_{55}, \text{right})$ taking \texttt{continue} yields $s=(s_{56}, \text{right})$.
    \\
    \textit{Example:} From $s=(s_{56}, \text{right})$ taking any action yields $s=(s_{46}, \text{right})$.
    
    \item The \texttt{dig} action allows the lemming to move down through a dirt tile (brown). 
    Taking the \texttt{dig} action over a non-dirt tile leads to the same result as if the \texttt{continue} action were taken.
    \\
    \textit{Example:} From $s=(s_{53}, \text{left})$ taking \texttt{dig} yields $s=(s_{43}, \text{left})$.
    \\
    \textit{Example:} From $s=(s_{43}, \text{left})$ taking any action yields $s=(s_{33}, \text{left})$.
\end{itemize}
The goal tile (yellow) and the water tiles (blue) are terminal states. The reward function returns $+100$ for entering the goal tile and $-100$ for entering a water tile. Assume $\gamma = 0.9$.

    \begin{center}
        \begin{tikzpicture}
        
            % Coloring specific cells
            \foreach \i in {1,...,9} {
                \fill[gray] (\i-1,5) rectangle (\i,6); % top row
                \fill[gray] (\i-1,0) rectangle (\i,1); % bottom row  
            }
            \foreach \j in {1,...,6} {
                \fill[gray] (0,\j-1) rectangle (1,\j); % left col
                \fill[gray] (8,\j-1) rectangle (9,\j); % right col  
            }
            \foreach \i in {3,...,5} {
                \fill[brown] (\i-1,3) rectangle (\i,4); 
            }
            \foreach \i in {6,...,7} {
                \fill[brown] (\i-1,2) rectangle (\i,3); 
            }
            \fill[brown] (1,3) rectangle (2,4);
            \fill[brown] (2,3) rectangle (3,4);
            
            \fill[blue] (4,0) rectangle (5,1);
            \fill[blue] (3,0) rectangle (4,1);  
            \fill[yellow] (7,1) rectangle (8,2);
            
            \fill[gray] (7,3) rectangle (8,4);
            \fill[gray] (8,3) rectangle (9,4);

            \node at (1.5,4.5) {\includegraphics[width=0.8cm]{figures/lemming.png}};
            
            \draw[step=1cm] (0,0) grid (9, 6);
            
            % Loop to place s_{ij} in each cell
            \foreach \i in {1,...,9} {
                \foreach \j in {1,...,6} {
                    % Places s_{ij} at the center of the cell (\i+1,\j+1)
                    \node at (\i+0.75-1,\j+0.75-1) {$s_{\j\i}$};
                }
            }
            
        \end{tikzpicture}
    \end{center}

\begin{subparts}

\subpart[1] \textbf{Numerical answer:} What is $V^*(s)$ for $s=(s_{27}, \text{right})$?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $+100$
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}

\clearpage
    
\subpart[1] \textbf{Numerical answer:} What is $V^*(s)$ for $s=(s_{27}, \text{left})$?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $0.9^2 * -100$
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}
    
\subpart[2] \textbf{Select one:} Suppose we are in a state $s=(s_{ij}, s_d)$ where $s_{ij} = s_{46}$. Is the action $a=$dig always an optimal action? \textbf{Briefly justify your answer.}
    \begin{checkboxes}
     \choice Yes
     \choice No
    \end{checkboxes}    
    \fillwithlines{8em}
    \begin{soln}
    No. If $s_d=$right, it is optimal. If $s_d=$left, it would cause the lemming to walk into the water, so $a=$continue should be chosen instead.
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}

\subpart[1] \textbf{Select one:} How many optimal policies are there for this problem?
    \begin{checkboxes}
     \choice zero
     \choice one
     \choice an integer greater than one
     \choice infinitely many
    \end{checkboxes}
    \begin{soln}
    an integer greater than one
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}

\subpart[1] \textbf{Numerical answer:} What is $Q^*(s,a)$ for $s=(s_{54}, \text{right})$ and $a=\texttt{dig}$?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $0.9^3 * -100 = -72.9$
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}

\subpart[1] \textbf{Select one:} Does this environment have stochastic transitions or deterministic transitions?
    \begin{checkboxes}
     \choice stochastic transitions
     \choice deterministic transitions
    \end{checkboxes}
    \begin{soln}
    deterministic transitions
    \end{soln}
    \begin{qauthor} Matt    \end{qauthor}
    
    
\end{subparts}


\part[2] \textbf{Select all that apply:} Which of the following is true of a Markov decision process (MDP)?
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice An MDP defines one way that the state, action, reward tuples are gathered for reinforcement learning.
     \choice The components of an MDP include a state space, action space, transition probabilities, reward function, and policy.
     \choice Every state/action space of finite size consists of exactly one optimal policy.
     \choice The Bellman equations are a recursive definition of the optimal policy.
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
    A, B
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}

\part[1] \textbf{True or False:} Value iteration learns the optimal value function from which we can infer the optimal policy. By contrast, policy iteration learns the optimal policy from which we \emph{cannot} infer the optimal value function.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False, we can infer the optimal value function from the optimal policy.
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}


\end{parts}