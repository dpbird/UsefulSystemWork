\begin{enumerate}
    
    \item  \pts{2} \textbf{True or False:} Suppose you flip a biased coin which always shows heads. 
    Then the entropy of the outcome of flipping this coin is $\log 2$.
    \textbf{Justify your answer.}
    \begin{enumerate}
    \item True
    \item False
    \end{enumerate}
    \fillwithlines{4em}
    
    \begin{soln} False. It is $0$ by definition.
    \end{soln}
    
    \begin{qauthor}
    Brynn- Taken from S17 10-601 Final. Touches on learning objectives 1 and 2
    \end{qauthor}
    
    
    \item\pts{2} \textbf{Select all that apply.} Which of the following statements are true?
    \begin{enumerate}
        \item As the number of data points grow to infinity, the MAP estimate approaches the MLE estimate, assuming we assign a non-zero prior probability to all possible values.

    
    \end{enumerate}
    \begin{soln}
    (b)True
    \end{soln} 
    
    \begin{qauthor}
    Brynn- Taken from F17 Final.  This is a little bit outside of the learning objectives, but I think is still relevant
    \end{qauthor}
    
    
    \item\pts{2} \textbf{Select all that apply.} Suppose you are given a task to train a ID3 decision tree classifier to predict whether a patient has cancer or not based on thousands of electronic medical records. Some of your classmates come to you, saying that their classifiers are better than yours. Which of the following reasons do you think are persuadable? 
    \begin{enumerate}
    \item My decision tree is better than yours because my training error is lower. 
    \item My decision tree is better than yours because my test error is lower. Footnote: The threshold for splitting a node according to the information gain in the trained decision tree model is 0.085635284963. 
    \item My decision tree is better than yours because my test error is lower. Footnote: The maximum allowed depth of the decision tree is optimized using a 10-fold cross-validation. 
    \item My decision tree is better than yours because my test error is lower. Footnote: The finally reported test errors are the ones with best splitting threshold and best maximum allowed depth of the decision tree. 
    \end{enumerate}
    
    \begin{soln}
    c. For a, should not use training error to evaluate the performance of the model. For b, with a hyper-parameter with that many decimal places, the model is very likely to be too fine tuned, which makes people suspect some information about the test data is used during the model training and tuning. For d, should not use test data to choose hyper-parameters. 
    \end{soln}
    
    \begin{qauthor}
    Mo- inspired by hw2 and adapted from 701 Midterm 2007 as some students did not fully digest the concept of cross validation, hyper-parameters, etc. 
    \end{qauthor}
    
    \item (True/False) We derived the mistake bound of the perceptron when the data is linearly separable. Suppose we have a set of data points where the center of these points is at the origin. (The center means the average of all the points.) Now suppose we translate each data point by the same amount and in the same direction. Then the mistake bound should stay the same after the translation.
    
    \begin{soln}
    False. Consider this counter-example, $x^{(1)} = \begin{bmatrix} 1 \end{bmatrix}, y^{(1)} = 1, x^{(2)} = \begin{bmatrix} -1 \end{bmatrix}, y^{(2)} = -1$. For this dataset, the algorithm makes 1 mistake before it converges. Now we translate the above data by $\begin{bmatrix} 3 \end{bmatrix}$, and the data becomes $x^{(1)} = \begin{bmatrix} 4 \end{bmatrix}, y^{(1)} = 1, x^{(2)} = \begin{bmatrix} 2 \end{bmatrix}, y^{(2)} = -1$. For this translated dataset, the algorithm makes 17 mistakes before it converges
    \end{soln}
    
    \begin{qauthor}
    Jennifer
    \end{qauthor}
    
        
    \item \textbf{True of False:} If function $f$ is convex, then $f$ cannot be concave.
    
    Remember for $f:\mathbb{R}^{M} \rightarrow \mathbb{R}$ and $\forall x_1 \in \mathbb{R}^{M}, x_2 \in \mathbb{R}^{M}, 0\leqslant t\leqslant 1$, \\ f is convex if 
    \begin{equation}
        f(tx_1 + (1-t)x_2) \leqslant tf(x_1) + (1-t)f(x_2);
    \end{equation} concave if 
    \begin{equation}
        f(tx_1 + (1-t)x_2) \geqslant tf(x_1) + (1-t)f(x_2);
    \end{equation}

    \begin{soln}
    False. A linear function is both convex and concave.
    \end{soln}
    
    \begin{qauthor}
    Qinghan
    \end{qauthor}
    
    
    \item The closed form solution to linear regression can be expressed as $w = \big( \mathbf{X}^T \mathbf{X}\big)^{-1}\mathbf{X}^Ty$. \textbf{True of False:} if $\mathbf{X}^T \mathbf{X}$ is not invertible, then an optimal solution to this linear regression task does not exist.
    
    \begin{soln}
    False. If $\mathbf{X}^T \mathbf{X}$ is not invertible, then there exist multiple optimal solutions.
    \end{soln}
    
    \begin{qauthor}
    Qinghan
    \end{qauthor}
    
    \item Select ONLY ONE Choice\\
    In this problem, we will derive the MAP estimation for the weights $\mathbf{w}$. Assume each element in $\mathbf{w}$ is independently drawn from a Gaussian distribution with zero mean and variance $\tau^{2}$, i.e., $w_{i} ~ \mathcal{N}(0,\tau^{2})$ (the dimension of $\mathbf{w}$ is $D$, we ignore bias here for convenience). We have the following observations, $\{\mathbf{x}^{(i)},y^{(i)}\}_{i=1}^{N}$. We assume each of the target values $y^{(i)}$ is drawn according to the distribution, $y^{(i)}\sim\mathcal{N}(\mathbf{w}^{T}\mathbf{x}^{(i)},\sigma^{2})$. The posterior distribution of w given the observation $\{\mathbf{x}^{(i)},y^{(i)}\}_{i=1}^{N}$ can be represented as $f(w|\{x^{(i)},y^{(i)}\}_{i=1}^{N};\tau,\sigma)$. Which of the following expression is proportional to this posterior distribution? Note $\tau$ and $\sigma$ are given nonnegative constants.\\
    (a) $\prod_{i=1}^{N}\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{w_{i}^{2}}{2\sigma^{2}}}\frac{1}{\sqrt{2\pi}\tau}e^{-\frac{\left(y^{(i)}-w^{T}x^{(i)}\right)^{2}}{2\tau^{2}}}$\\
    \begin{soln}
    (b) $\Pi_{i=1}^{N}e^{-\frac{(y-(w_{0}+\mathbf{w}^{T}\mathbf{x}^{(i)}))^{2}}{2\sigma^{2}}}\times\Pi_{j=1}^{D}e^{-\frac{w_{j}^{2}}{2\tau^{2}}}$\\
    \end{soln}

    (c) $\Pi_{i=1}^{N}e^{-\frac{(y-(w_{0}+\mathbf{w}^{T}\mathbf{x}^{(i)}))^{2}}{2\sigma^{2}}}$\\

    (d) $\Pi_{j=1}^{D}e^{-\frac{w_{j}^{2}}{2\tau^{2}}}$\\
    
    \begin{qauthor}
    Yu Huang
    \end{qauthor}
    
    \item Select ONLY ONE Choice\\
    To avoid overfitting, we often add a $l_{2}$ regularizer to the objective of a linear regression problem, i.e., the optimization problem becomes $\underset{w}{min}\text{ }||\mathbf{X}\mathbf{w}-Y||_{2}^{2}+\lambda||\mathbf{w}||^{2}$ where $\lambda$ is a positive constant. Similar to the original problem, we can derive the closed form solution to the problem:

    The loss function becomes $L=(\mathbf{X}\mathbf{w}-Y)^{T}(\mathbf{X}\mathbf{w}-Y)+\lambda\mathbf{w}^{T}\mathbf{w}$

    First we take the derivative of the objective function with respect to w and set it to zero, arriving at equation (???). 

    Finally, we arrive at the solution $\mathbf{w}=(\mathbf{X}^{T}\mathbf{X}+\lambda I)^{-1}\mathbf{X}^{T}Y$

    What should equation (???) be?\\
    (a) $(\mathbf{X}\mathbf{w}-Y)^{T}\mathbf{X}+\lambda\mathbf{w}=0$\\
    \begin{soln}
    (b) $(\mathbf{X}\mathbf{w}-Y)^{T}\mathbf{X}+\lambda\mathbf{w}^{T}=0$\\
    \end{soln}
    (c) $(\mathbf{X}\mathbf{w}-Y)^{T}\mathbf{X}+\lambda\mathbf{w}^{T}\mathbf{X}^{T}=0$\\
    (d) $(\mathbf{X}\mathbf{w}-Y)^{T}\mathbf{X}+2\lambda\mathbf{w}=0$\\
    \begin{qauthor}
    Yu Huang
    \end{qauthor}
    
    
    \item Consider N training points $(x_{i},y_{i})$ drawn i.i.d from a distribution that is characterized as:

    $$x_{i}\sim h(x)$$ 
    
    $$y_{i}=f(x_{i})+\epsilon_{i}$$
    
    $$\epsilon_{i}\sim\mathcal{N}(0,\sigma^{2})$$
    
    Where $f$ is the regression function. An estimator for this data, linear in $y_{i}$ is given by:
    
    $$\hat{f}(x^{\star})=\sum_{i=1}^{N}l_{i}(x^{\star};\mathcal{X})y_{i}$$
    
    where $l_{i}(x^{\star};\mathcal{X})$depends on the entire training sequence of $x_{i}(denoted by \mathcal{X})$ but do not depend on $y_{i}$. Show that k-nearest-neighbor regression and linear regression are members of this class of estimators. What would be the $l_{i}(x^{\star};\mathcal{X})$. in both these regressions (knn and linear)?
    
    Hint: k-nearest-neighbor regression here means KNN weighted by inverse Euclidean distance. 
    
    \begin{soln}
    1. KNN regression

    If we use uniform weights then
    
    $l_{i}=\frac{1}{K}$ which is irrelevant to $\mathbf{y}$
    
    If we use inverse distance weights for example Euclidean distance then
    
    $l_{i}=\frac{\lVert x^{*}-x_{i}\lVert_{2}^{-1}}{\sum_{t=1}^{K}\lVert x^{*}-x_{i}\lVert_{2}^{-1}}$ which is also irrelevant to $\mathbf{y}$
    
    2. Linear Regression
    
    Like common linear regression, we minimize the mean squre error
    
    $$argmin_{w}\frac{1}{N}\sum_{i=1}^{N}(y_{i}-wx_{i})^{2}$$
    
    solving this we obtain $w=\frac{\sum_{i=1}^{N}x_{i}y_{i}}{\sum_{i=1}^{N}x_{i}^{2}}$ and use this to predict the target value for $x^{*}$we obtain:
    
    $\hat{f}(x^{*})=\sum_{i=1}^{N}\frac{x^{*}x_{i}y_{i}}{\sum_{i=1}^{N}x_{i}^{2}}$ where we see that $l_{i}=\frac{x^{*}x_{i}}{\sum_{i=1}^{N}x_{i}^{2}} $
    \end{soln}
    
    \begin{qauthor}
    Yu Huang adapted from 10707 fall 2017 homework 1.
    \end{qauthor}
    
    
    \item Assume we have $N$ training samples $\xv^{(1)}, \xv^{(2)},..., \xv^{(N)} $ with binary labels $y^{(1)}, y^{(2)},..., y^{(N)}$.
    For Binary Logistic Regression, derive $J(\thetav)$, $\nabla_{\thetav} J(\thetav)$.
    
    \begin{soln}
        \begin{align*}
        J(\thetav) 
        &= - \frac{1}{N} \sum_i \log \mathbb P (y^{(i)} \vert \xv^{(i)}, \thetav) \\
        &= - \frac{1}{N} \sum_i y^{(i)} (\thetav^\top \xv^{(i)}) - \log ( 1 + e^{\thetav^\top \xv^{(i)}}) \\
        \nabla J(\thetav)
        &= -\frac{1}{N} \sum_i \xv^{(i)} \Big( y^{(i)} - \frac{e^{\thetav^\top \xv^{(i)}}}{1 + e^{\thetav^\top \xv^{(i)}}} \Big)
        \end{align*}
    \end{soln}
    
    \begin{qauthor}
    Boyue Li (adapted from S18 HW4).
    \end{qauthor}
    
    \item Consider drawing a ball repeatedly with replacement from a bag of blue and red balls. The balls drawn in five trials are [blue, blue, red, red, blue]. What is the most likely proportion of blue balls in the bag? Show your work.
    
    \begin{soln}
    Let $p$ be the probability of drawing a blue ball from the bag.
    \begin{align*}
        L(X;p) =&\ \prod_{i=1}^n p(x^{(i)} | p)\\ 
        =&\ p^3(1-p)^2\\ 
    \end{align*}
    Set $\frac{\partial L}{\partial p} = 0$, we have $p_{MLE} = \frac{3}{5}$
    \end{soln}
    
    \begin{qauthor}
    Elaine Liu, ML as optimization 3a (adapted from Math Background Test)
    \end{qauthor}
    
    
    \item Recall the decision function for perceptron is $h(x) = sign(\theta^T x)$ and for logistic regression is $P_{\theta} (y=1|x) = \frac{1}{1 + exp(-\theta^T x)}$. 
    (a) Plot a graph for each decision function, clearly showing its shape, axes and possible range of values. 
    
    (b) Why can we use gradient descent for training in logistic regression, but not for perceptron?
    
    \begin{soln}
    (a) 
    
    \includegraphics[width=6cm]{fig/perceptron_plot.jpg}
    Horizontal axis: $\theta^T x$. Vertical axis: $h(x)$
    
    \includegraphics[width=6cm]{fig/LR_plot.jpg}
    Horizontal axis: $\theta^T x$. Vertical axis: $P (y=1|x)$
    
    (b) The decision function for perceptron is not differentiable.
    \end{soln}
    
    \begin{qauthor}
    Elaine Liu, ML as optimization 2d (inspired by lecture on log reg)
    \end{qauthor}
    
    
\item We are doing a logistic regression for image classification with K classes and as usual, we can augment each feature vector $\xv^{(i)}$ (M - dimensional) with a bias feature that always takes value 1 (this bias feature will be treated just like any other feature), resulting in the following expression for the likelihood:

 \begin{equation}
   \mathbb P(y^{(i)} = k \vert \mathbf x^{(i)})
    = \frac {\exp({{\mathbf \thetav^{(k)}}^\top \mathbf x^{(i)}})} {\sum_{j=1}^{K} \exp({{\mathbf \thetav^{(j)}}^\top \mathbf x^{(i)}})}
    \label{eq:logreg}
\end{equation}
The weight matrix $\mathbf{\theta}$ would be a $K\times(M+1)$ matrix. While performing SGD the gradient update will therefore be as follows:

\begin{equation}
    \thetav^{(k)} \leftarrow \thetav^{(k)} - \eta  \Bigg[\nabla_{\mathbf \thetav^{(k)}} J^{(i)}(\mathbf \thetav)\Bigg]
\end{equation}

If we were to perform regularization would this update equation change and why?
    \begin{soln}
    The bias term would get regularized if we dont change the update \\
    \end{soln}
    \begin{qauthor}
    Sriram
    \end{qauthor}


    \item \textbf{True or false:} We can use a sigmoid activation for the output layer in case of a binary classification problem?
\begin{enumerate}
\item True
\item False
\end{enumerate}

    \begin{soln}
    True 
    \end{soln}
    
    \begin{qauthor}
    Eti 
    \end{qauthor}
    
    \item \textbf{True or false:} Suppose we have a neural network with ReLU activation function. Let’s say, we replace ReLU activations with linear activations. Would this new neural network be able to approximate an XNOR function?
    
    \begin{enumerate}
    \item True
    \item False
    \end{enumerate}

    \begin{soln}
    False - If ReLU activation is replaced by linear activation, the neural network loses its power to approximate non-linear function.

 
    \end{soln}
    
    \begin{qauthor}
    Eti  
    \end{qauthor}
    
    
    \item The number of nodes in the input layer is 10 and the hidden layer is 5. The maximum number of connections from the input layer to the hidden layer is:
\begin{enumerate}
 \item 50

 \item More than 50

 \item Less than 50

  \item Arbitrary value
 \end{enumerate}    
    
    
     \begin{soln}
    
      A
 
    \end{soln}
    
    \begin{qauthor}
    Abhijeet 
    \end{qauthor}
    
    
        \item     Which of following activation function can’t be used at output layer for classification:
    \begin{enumerate}
     \item sigmoid
    
     \item tanh
    
     \item ReLU
    
      \item softmax
     \end{enumerate}    
    
    
     \begin{soln}
    
      C
 
    \end{soln}
    
        \begin{qauthor}
    Abhijeet 
    \end{qauthor}
    
    
   \item Statement 1: It is possible to train a network well by initializing all the weights as 0.
Statement 2: It is possible to train a network well by initializing biases as 0.

\textbf{Which of the statements given above is true?}
    
\begin{enumerate}
     \item Statement 1 is true while Statement 2 is false
    
     \item Statement 2 is true while Statement 1 is false
    
     \item Both statements are true
    
      \item Both statements are false
     \end{enumerate}    
    
    
     \begin{soln}
    
      B
 
    \end{soln}
    
        \begin{qauthor}
    Abhijeet 
    \end{qauthor}
    
    
    \item Suppose you are given a dataset with 4 binary features and 2 possible labels. You decide to construct a small neural network using one hidden layer with 2 hidden units using the sigmoid activation function and 2 output units using the softmax activation function.
    
    We'll call the matrix of weights from the input to the hidden layer $\alpha$ and the matrix of weights from the hidden layer to the output $\beta$.
    
    You initialize the weights as:
    
    \begin{equation*}
        \alpha = \begin{bmatrix}
        1 & -1 & 0.5 & 1 \\
        -1 & 0.5 & 1 & -0.5
        \end{bmatrix}
    \end{equation*}
    \begin{equation*}
        \beta = \begin{bmatrix}
        1 & 0\\
        0 & 1
        \end{bmatrix}
    \end{equation*}
    
    and the biases as 1.
    
    Your first training example is $x^{(1)}=(0, 1, 1, 0)$ and $y^{(1)}=1$.
    
    Let us call the values calculated by the hidden nodes $z_i$ and those calculated by the output nodes $\hat{y}_i$.
    
    Running feed forward on example $x^{(1)}$, fill in the blanks:
    
    $z^{(1)}_1 = \sigma(\underline{\hspace{1cm}})$\\
    $z^{(1)}_2 = \sigma(\underline{\hspace{1cm}})$\\
    
    Write the following in terms of $z^{(1)}_1$ and $z^{(1)}_2$
    
    $\hat{y}^{(1)}_1:$\\
    
    
    $\hat{y}^{(1)}_2$:\\
    
    \begin{qauthor}
    Sarah
    \end{qauthor}
    
    
\end{enumerate}

\section{Linear Models}

% =================== Gold-Ratio Model ==================
Imagine that you are inventing a more aesthetically appealing machine learning model for binary classification. The constant $e =\sum_{n=0}^{\infty} \frac{1}{n!}  \approx 2.71828$ appears frequently throughout machine learning. You decide to swap out $e$ for something better: namely, the constant $\varphi = \frac{1 + \sqrt{5}}{2} \approx 1.61803$ representing the value of the golden ratio, said by many to be essential to aesthetics in architecture and art.

Accordingly, you define a probability distribution as follows:

\begin{align} 
p_{\wv, b}(y = 1 | \xv) = \frac{ 1 }{ 1 + \varphi^{- (\wv^T \xv + b)} }
\end{align}

where $\wv = [w_1, \ldots, w_M]$ and $b$ are the model parameters. You call your creation \emph{binary golden-ratio logistic regression}.
% ==========================================================
