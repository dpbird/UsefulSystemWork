\sectionquestion{S23 TA Questions Go Here!} 

\begin{parts}


\part \textbf{Select all that apply:} for each of the following algorithms, select the correct update rules.

\begin{subparts}
\subpart[1] SGD for Linear Regression: $h_{\theta}(\xv) = \theta^T \xv$\\
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice $\theta_k \leftarrow \theta_k + \theta^T(\x^{(i)} - y^{(i)})$
     \choice $\theta_k \leftarrow \theta_k + \frac{1}{1 + \exp \alpha(\theta^T x^{(i)} - y^{(i)})}$
     \choice $\theta_k \leftarrow \theta_k + \alpha(\theta^T x^{(i)}-y^{(i)})x_k^{(i)}$
     \choice $\theta_k \leftarrow \theta_k + \alpha \left(\frac{1}{N} \sum_{i=1}^{N} \left(\theta^Tx^{(i)} - y^{(i)}\right)x_k^{(i)}  \right)$
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
    C
    \end{soln}
    \begin{qauthor}
    Author: Tara\\
    Learning Objective(s): \\
    Source: Matt Gormley S23 10-601 Lecture 10 Slides
    \end{qauthor}
    
\subpart SGD for Logistic Regression: $h_{\theta}\xv = p(x|y)$\\
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice $\theta_k \leftarrow \theta_k + p(\x^{(i)} - y^{(i)}|y^{(i)})$
     \choice $\theta_k \leftarrow \theta_k + \frac{1}{1 + \exp \alpha(p(x^{(i)}| y^{(i)}) - y^{(i)} )}$
     \choice $\theta_k \leftarrow \theta_k + \alpha(p(x^{(i)}|y^{(i)})-y^{(i)})x_k^{(i)}$
     \choice $\theta_k \leftarrow \theta_k + \alpha \left(\frac{1}{N} \sum_{i=1}^{N} \left(p(x^{(i)}|y^{(i)}) - y^{(i)}\right) x_k^{(i)}  \right)$
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
    C
    \end{soln}
    \begin{qauthor}
    Author: Tara\\
    Learning Objective(s): \\
    Source: Matt Gormley S23 10-601 Lecture 10 Slides
    \end{qauthor}
\subpart Perceptron: $h_{\theta} = \text{sign}(\theta ^T \xv)$ where $\text{sign}(\cdot) \in \{-1, +1\}$ \\
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice $\theta_k \leftarrow \theta_k + \text{sign}(\theta^T(\x^{(i)} - y^{(i)}))$
     \choice $\theta_k \leftarrow \theta_k + \frac{1}{1 + \exp \alpha(\text{sign}(\theta^Tx^{(i)}) - y^{(i)})}$
     \choice $\theta_k \leftarrow \theta_k + \alpha(\text{sign}(\theta^Tx^{(i)})-y^{(i)})x_k^{(i)}$
      \choice $\theta_k \leftarrow \theta_k + \alpha \left(\frac{1}{N} \sum_{i=1}^{N} \left(\text{sign}(\theta^Tx^{(i)}) - y^{(i)}\right)x_k^{(i)}  \right)$
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
    C
    \end{soln}
    \begin{qauthor}
    Author: Tara\\
    Learning Objective(s): \\
    Source: Matt Gormley S23 10-601 Lecture 10 Slides\\
    \end{qauthor}
    \begin{qtester}
    Should clarify that $\text{sign}(\cdot)$ is 0/1.\\
    Tara: Updated
    \end{qtester}
\end{subparts}


\part [3] \textbf{Short answer:} Suppose we are given $n$ training points $(x^{(i)},y^{(i)})_{i=1}^n$ such that $x^{(i)} \in [-1,1]$ and $y^{(i)} \in \{-1,1\}$. Additionally, we know that whenever $ -1 \le x^{(i)} \le -0.5$, $y^{(i)} = 1$, whenever $ -0.5 \le x^{(i)} \le 0.5$, $y^{(i)} = -1$ and whenever $ 0.5 \le x^{(i)} \le 1$, $y^{(i)} = 1$. The current dataset is not linearly separable. However, if we define a function $f: [-1,1] \to \mathbb{R}^2$, this dataset can be linearly separated in $\mathbb{R}^2$. This means that the function $f$ maps a scalar $x \in [-1,1]$ to a vector in $\mathbb{R}^2$ and that using this function on the dataset would mean that the modified dataset becomes: $(f(x^{(i)}),y^{(i)})_{i=1}^n$ such that $f(x^{(i)}) \in \mathbb{R}^2$ and $y^{(i)} \in \{-1,1\}$ There are many such functions which can linearly separate the modified dataset. Define one such $f$. Also, define the corresponding decision boundary in terms of the weight vector $\theta  = [\theta_1, \theta_2]^T\in \mathbb{R}^2$
    \fillwithlines{6em}
    \begin{soln}
      There are many answers possible, one such answer is $f(x) = [x,x^2]^T$, the corresponding decision boundary is $\theta_2 = 0.25$
    \end{soln}

    \begin{qauthor}
    Pranit,  Convert linearly inseparable dataset to a linearly separable dataset in higher
dimensions
    \end{qauthor}

    \begin{qtester}
     A lot to read, and a bit difficult to grade.
    \end{qtester}


\part[2] \textbf{Select all that apply:} Which of the following statements are true about deriving the PDF and CDF of the minimum and maximum of independent random variables $X_1, X_2, \dots, X_n$? Assume that $f_{X_i}(x)$ is the PDF of $X_i$ and $F_{X_i}(x)$ is the CDF of $X_i$.
\checkboxchar{$\Box$} \checkedchar{$\blacksquare$}
\begin{checkboxes}
    \choice The CDF of the minimum is given by $F_{\min}(x) = 1 - \prod_{i=1}^n (1-F_{X_i}(x))$, where $F_{X_i}(x)$ is the CDF of $X_i$.
    \choice The PDF of the minimum is given by $f_{\min}(x) = n \cdot \sum_{i=1}^n f_{X_i}(x) \cdot \prod_{j \neq i} (1-F_{X_j}(x))$.
    \choice The CDF of the maximum is given by $F_{\max}(x) = \prod_{i=1}^n F_{X_i}(x)$.
    \choice The PDF of the maximum is given by $f_{\max}(x) = n \cdot F_{\max}(x)^{n-1} \cdot \prod_{i=1}^n f_{X_i}(x)$.
    \choice None of the above
\end{checkboxes}
\begin{soln}
    A, B, and C are correct. D is incorrect.
    
\end{soln}
\begin{qauthor}
    Yash, Recall probability basics 
\end{qauthor}

    \begin{qtester}
    nice
     We should write out "cumulative distribution function (CDF)" and "probability distribution function (PDF)" when first using the abbreviations. 
    \end{qtester}


\part Consider a linear regression model with a single predictor variable, where the response variable $y$ is modeled as a linear function of the predictor variable $x$, with an intercept term $\beta_0$ and a slope term $\beta_1$. Let $\epsilon$ denote the error term, assumed to be normally distributed with mean zero and variance $\sigma^2$. The model can be written as:
\begin{equation}
    y = \beta_0 + \beta_1 x + \epsilon
\end{equation}
Suppose we have a dataset with $n$ observations of the response variable $y$ and the predictor variable $x$. 

\begin{subparts}
   \subpart[1] \textbf{Derivation:} Write down the log likelihood function for this model, conditional on the given dataset.
    
    \begin{tcolorbox}[fit,height=5cm, width=15cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    
    \subpart[2] \textbf{Derivation:} Show that maximizing the likelihood function is equivalent to minimizing the sum of squared errors.
    
    \begin{tcolorbox}[fit,height=5cm, width=15cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    


\end{subparts}
\begin{soln}
    \begin{enumerate}
        \item The likelihood function for the linear regression model is given by:
        $$ L(\beta_0, \beta_1, \sigma^2 | y_1, y_2, \ldots, y_n, x_1, x_2, \ldots, x_n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2\sigma^2}\right) $$
        Writing the log likelihood,
        $$ \log L = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 $$
        
        \item The sum of squared errors (SSE) for the linear regression model is given by: $$ SSE = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 $$
        Writing the log likelihood,
        $$ \log L = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2 $$ 
        Maximizing the likelihood function is equivalent to minimizing the negative logarithm of the likelihood function, which is proportional to SSE. Therefore, maximizing the likelihood function is equivalent to minimizing SSE.

    \end{enumerate}
\end{soln}
\begin{qauthor}
    Yash, MLE to learn the parameters and in Linear Regression MLE $<=>$ MSE
\end{qauthor}
\begin{qtester}
    I get what this question is going for but the set up needs more detail. For example we should start with the guassian distribution and structure the instructions around that.
\end{qtester}


\part Suppose we are asked to perform linear regression on a design matrix of features $\Xv \in \mathbb{R}^{n \times d}$ and a vector of labels $\yv \in \mathbb{R}^n$ to find the optimal weight vector $\wv \in \mathbb{R}^d$. \textit{Assume the intercept term is folded into the model}.

\begin{subparts}
    \subpart \textbf{Short answer:} What is the expression for the mean squared error objective in terms of $\Xv, \yv, \wv, n$? Do not use any indices or summations in your answer.
    \begin{tcolorbox}[fit,height=1.5cm, width=6cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $\frac{1}{n} \|\Xv \wv - \yv \|_2^2$
    \end{soln}   

    \subpart \textbf{Short answer:} What is the closed-form solution for the $\wv$ that best optimizes mean squared error over the dataset?
    \begin{tcolorbox}[fit,height=1.5cm, width=6cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $\wv = (\Xv^\top \Xv)^{-1} \Xv^\top \yv$
    \end{soln}

    \subpart \textbf{Short answer:} Suppose we add an L2 regularization penalty that regularizes \textit{every parameter in $\wv$} with regularization parameter $\lambda$ to the objective. What is the expression for the objective in terms of $\Xv, \yv, \wv, n, \lambda$? Do not use any indices or summations in your answer.
    \begin{tcolorbox}[fit,height=1.5cm, width=6cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $\frac{1}{n} \|\Xv \wv - \yv \|_2^2 + \lambda \|\wv \|_2^2$
    \end{soln}   

    \subpart \textbf{Short answer:} Suppose we add an L2 regularization penalty with regularization parameter $\lambda$ to the objective. What is the closed-form solution for the $\wv$ that best optimizes our new objective?
    \begin{tcolorbox}[fit,height=1.5cm, width=6cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $\wv = (\Xv^\top \Xv + \lambda \Iv)^{-1} \Xv^\top \yv$
    \end{soln}

    \subpart \textbf{Short answer:} What problem may arise with the proposed regularization strategy if we were to shift every value in $\yv$ by a large constant factor?
    \fillwithlines{6em}
    \begin{soln}
        We regularized the intercept, so the model is not invariant to this shift and may perform poorly.
    \end{soln}
\end{subparts}
\begin{qauthor}
    Abhi,

    Implement learning for Linear Regression using three optimization techniques: (1)
closed form, (2) gradient descent, (3) stochastic gradient descent

Apply knowledge of zero derivatives to identify a closed-form solution (if one exists)
to an optimization problem


Add a regularizer to an existing objective in order to combat overfitting

Explain why we should not regularize the bias term
\end{qauthor}\\

\begin{qtester}
    I feel like a lot of students will just have these equations on their cheat sheets and not necessarily have to understand the question to get it. It also wasn't clear to me that we were regularizing the bias term. I feel like a lot of people write regularization like this but assume that we aren't regularizing the $w_0$ term.
\end{qtester}



\part[2] \textbf{Select all that apply:} In which of the following cases would it be appropriate to use a Naive Bayes Classifier?
    {%
        \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
        \begin{checkboxes}
         \choice Poem classification, where you want to predict if a sentence is a poem based on the words in it
         \choice Spam filtering, where you want to predict if an email is spam or not based on its contents
         \choice Stock prediction, where you want to predict if a stock will go up in the future based on its previous values
         \choice News article categorization, where you want to predict the category that a news article belongs to (examples of categories are sport, politics etc).
        \end{checkboxes}
    }
    \begin{soln}
        B,D
    \end{soln}
    \begin{qauthor}
        Advaith, Check if students understand that the conditional independence assumption of Naive Bayes
    \end{qauthor}
    
    \begin{qtester}
        I think students could argue that you could use NB for all of these. Unless i am missing something. The independancy assumption doesn't have to hold for the model to still be successful.
    \end{qtester}


    


\part \textbf{Select all that apply:} Which of the following is true? Use the lines given below to justify your answer in not more than two sentences.

{%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice In a single hidden layer with linear activation function, the bias term can be folded into the input by appending a 0 to the input vector. 
     \choice In a single hidden layer with linear activation function, the bias term can be folded into the input by appending a 1 to the input vector.
     \choice None of the above 
    \end{checkboxes}

    \fillwithlines{6em}
}
\begin{soln}
    B, Reason: It is only valid to append 1 since it acts as a fixed input value that is multiplied by corresponding weight matrix values. It thus allows the bias term to be included in the linear combination.
\end{soln}
\begin{qauthor}
    Tanvi, Check if students understood how to operationalise incorporating the bias into the neural network.
\end{qauthor}
\begin{qtester}
    update 'single linear layer' to be "single hidden layer with linear activation function'
    I don't understand what C and D are saying? If C is correct then surely D is also correct. The initialization of the weight matrix doesn't matter.

    Spelling of Initializing.

    Tanvi: I realised it's too confusing, so I modified the question to only focus on the first two options and require an explanation.
\end{qtester}


\part \textbf{Select all that apply:} Which of the following Gaussian Naive Bayes classifiers will have a linear decision boundary? We have two features that are drawn from a Gaussian distribution.
    {%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
     \choice A classifier with binary labels, where the features have equal means and equal variances.
     \choice A classifier with binary labels, where the features have unequal means but equal variances.
     \choice A classifier with binary labels, where the features have unequal means and unequal variances.
     \choice A classifier with discrete, categorical labels, where the features have equal means but unequal variances.
     \choice A classifier with discrete, categorical labels, where the features have equal means and equal variances.
    \end{checkboxes}
    }
\begin{soln}
    A, B, E. The Gaussian should have equal variances.
\end{soln}

\begin{qauthor}
    Emaan, Learning Objective: Describe how the variance affects whether a Gaussian Naive Bayes model will have a linear or nonlinear decision boundary.
\end{qauthor}

\begin{qtester}
    Seems fine, its a case of if you know that the variances effect the linear/non-linear decision boundaries then this is easy. Should we maybe ask this in a short answer question instead of having all of the reading and guessing? It would be less reading time for students but more grading time for us.  i could go either way on this
\end{qtester}

\end{parts}