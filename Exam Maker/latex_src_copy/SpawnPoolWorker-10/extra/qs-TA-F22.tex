\sectionquestion{F22 TA Questions Go Here!}

\begin{parts}

% \part TODO: change these to questions about a separate regression model from the above question
% \begin{subparts}
%         \subpart \textbf{Select all that apply:} Polly suggests that Model 2 is a universal approximator. Neural disagrees. Of the following functions, select all which \textbf{cannot} be modeled with arbitrarily low true error by Model 2.
%             {%
%             \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
%             \begin{checkboxes}
%              \choice $f(\xv) = \text{sign} \left(- \xv_d \right)$, where $\xv_d$ is the last element of $\xv$
%              \choice $f(\xv) = \text{sign} \left( \sin (\xv_d)) \right)$, where $\xv_d$ is the last element of $\xv$
%              \choice $f(\xv) = \text{sign} \left( \| \xv \|_2^2 \right)$
%              \choice $f(\xv) = \text{sign} \left( 1 \right)$
%              \choice None of the above
%             \end{checkboxes}
%             }
%             \begin{soln}
%             B, C
%             \end{soln}
%         \subpart \textbf{Select one:} Cilantro suggests that adding sigmoid in the model as follows to create \textit{Model 3:} 
%         $$\mathcal{H}_3 = \{ f(x) = \beta^T (\alpha \sigma(x)) + \theta^T \sigma(x) + b \; | \; \beta \in \mathbb{R}^m, \alpha \in \mathbb{R}^{m \times n}, b \in \mathbb{R} \},$$ 
%         will make it a universal approximator. Is Cilantro right?
%             \begin{checkboxes}
%             \choice Yes, a non-linear activation function is sufficient to make the model a universal approximator
%             \choice No, a hidden dimension of arbitrary size is also necessary to create a universal approximator
%             \choice Yes, because Model 2 was already a universal approximator, and is still one regardless of the activation function applied to it.
%             \choice No, $\tanh$ is the only activation function which can be used to create a universal approximator according to the Universal Approximation Theorem.
%             \choice None of the above
%             \end{checkboxes}
%             \begin{soln}
%             B
%             \end{soln}
% \end{subparts}


%     \begin{qauthor}
%     Abhi \& Tara\\
%      Identify (some of) the options available when designing the architecture of a neural network\\
    
%     PAC 1 a .Identify the properties of a learning setting and assumptions required to ensure low generalization error

%     PAC 1 b Distinguish true error, train error, test error

%     \end{qauthor}

 
\end{parts}
