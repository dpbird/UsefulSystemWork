\sectionquestion{Optimization, Regularization, and Modeling}

\begin{parts}


% Question 5
\part Given a dataset with \( n \) points \((\xv^{(i)}, y^{(i)})\) where \( \xv^{(i)} \) is the feature vector of the $i$th point and \( y^{(i)} \) is the corresponding true output, a linear regression model predicts the output as \( \hat{y}^{(i)} = \wv^T \xv^{(i)} + b \), where \( \wv \in \Rb^M \) is the weight vector and \( b \in \Rb \) is the bias. The L2 regularized Mean Squared Error (MSE) objective function is defined as:
    \begin{align*}
        J_{\text{MSE}}(\wv, b) &= \left( \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)})^2 \right)   \\
        J_{\text{reg}}(\wv, b) &= J_{\text{MSE}}(\wv, b) + \lambda \|\wv\|^2
    \end{align*}
    where \( \lambda \) is the regularization parameter that controls the amount of regularization, and \( \|\wv\|^2 \) is the L2 norm of the weight vector \( \wv \).
    

\begin{subparts}

\subpart[2] \textbf{Short Answer:} Derive \( \frac{\partial J_{\text{reg}}}{\partial \wv} \), i.e. the partial derivatives of \( J_{\text{reg}}(\wv, b) \) with respect to \( \wv \).
    %
    \emph{Hint:} Note that $\frac{\partial J_{\text{MSE}}}{\partial \wv} = \frac{2}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)}) \xv^{(i)}$

    \begin{tcolorbox}[fit,height=5cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \end{tcolorbox}

    \begin{soln}
    \begin{align*}
        %\frac{\partial J_{\text{reg}}}{\partial w} &= \frac{\partial}{\partial w} \left( \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)})^2 \right) + \frac{\partial}{\partial w} (\lambda \|w\|^2) \\
        %\frac{\partial}{\partial w} \left( \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)})^2 \right) &= \frac{2}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)}) \xv^{(i)} \\
        \frac{\partial}{\partial w} (\lambda \|w\|^2) &= \frac{\partial}{\partial w} (\lambda w^T w) = 2 \lambda w \\
        \frac{\partial J_{\text{reg}}}{\partial w} &= \frac{2}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)}) \xv^{(i)} + 2 \lambda w
    \end{align*}
    
    \end{soln}

\subpart[2] \textbf{Short Answer:} Derive \( \frac{\partial J_{\text{reg}}}{\partial b} \), i.e. the partial derivatives of \( J_{\text{reg}}(\wv, b) \) with respect to \( b \).

    \begin{tcolorbox}[fit,height=5cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \end{tcolorbox}

    \begin{soln}
    \[
    \frac{\partial J_{\text{reg}}}{\partial b} = \frac{\partial}{\partial b} \left( \frac{1}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)})^2 \right) = \frac{2}{n} \sum_{i=1}^{n} (\hat{y}^{(i)} - y^{(i)})
    \]
    \end{soln}

\clearpage

\subpart[2] \textbf{Short answer:} Why do we generally avoid regularizing the bias term $b$ in linear regression?
    \fillwithlines{8em}
    \begin{soln}
    We regularize only the weight vector \( w \) and not the bias term \( b \). The reason is that the bias term is responsible for adjusting the output independently of the input features, allowing the model to fit the data's average level better. Regularizing the bias term would restrict this adjustment, potentially leading to a model that fits the data less accurately, especially when the data does not center around the origin. 
    \end{soln}
    
    \begin{qauthor}
    Monica. 4(d) Add a regularizer to an existing objective in order to combat overfitting. 4(e) Explain why we should not regularize the bias term.
    \end{qauthor}

    
\subpart[2] \textbf{Select all that apply:} Our objective function, L2 regularized MSE, is strictly convex. Which of the following is true?
    \begin{checkboxessquare}
     \choice Solving for the optimal parameters in closed form will always be more computationally efficient than solving for them with gradient descent.
     \choice The objective has a unique global minimum and gradient descent converges to that minimum, if it converges.
     \choice Stochastic gradient descent and gradient descent will typically converge to different local minima of the function.
     \choice Nonce of the above
    \end{checkboxessquare}
    \begin{soln}
    The objective has a unique global minimum and gradient descent converge to that minimum, if it converges.
    \end{soln}
    \begin{qauthor}
    Matt
    \end{qauthor}
    

\end{subparts}



\end{parts}