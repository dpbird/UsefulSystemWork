\sectionquestion{Neural Networks \& Backpropagation}

\begin{parts}


% Question 3 and 6 merged into one
\part Consider the neural network with $1$ hidden layer shown below for a binary classification problem, where $\xv \in \Rb^3$ is the input feature vector and $\yv \in \Rb^2$ is a one-hot vector representing the correct class. Note: this network does not contain bias terms.

%
\begin{minipage}{0.5\linewidth}
%\begin{figure}[h]
    \def\distH{2.5cm}
    \def\distHTwo{2.0cm}
    \def\distHThree{0.3cm}
    \def\distV{0.6cm}
    \def\distVTwo{0.3cm}
    \centering
    \begin{tikzpicture}[
        > = stealth, % arrow head style
        shorten > = 0pt, % don't touch arrow head to node
        auto,
        % node distance = 2.5cm, % distance between nodes
        thick % line style
    ]\footnotesize
    \tikzstyle{every state}=[
        draw = black,
        thick,
        fill = white,
        minimum size = 0.8cm,
    ]
    \node[state] (X1){$x_1$};
    \node[state] (X2) [below = \distV of X1] {$x_2$};
    \node[state] (X3) [below = \distV of X2] {$x_3$};
    \node[state] (Z1) [above right = \distVTwo and \distH of X2] {$z_1$};
    \node[state] (Z2) [below = \distV of Z1] {$z_2$};
    \node[state] (y1) [right = \distHTwo of Z1] {$\hat{y}_1$};
    \node[state] (y2) [right = \distHTwo of Z2] {$\hat{y}_2$};

    \path[->] (X1) edge node  [above, near start, font=\scriptsize]{$\alpha_{1,1}$} (Z1);
    \path[->] (X1) edge node [above, near start, font=\scriptsize]{$\alpha_{2,1}$} (Z2);
    \path[->] (X2) edge node [above, near start, font=\scriptsize]{$\alpha_{1,2}$} (Z1);
    \path[->] (X2) edge node [above, near start, font=\scriptsize]{$\alpha_{2,2}$} (Z2);
    \path[->] (X3) edge node [above, near start, font=\scriptsize]{$\alpha_{1,3}$} (Z1);
    \path[->] (X3) edge node [above, near start, font=\scriptsize]{$\alpha_{2,3}$} (Z2);
    \path[->] (Z1) edge node [above,  near start, font=\scriptsize] {$\beta_{1,1}$} (y1);
    \path[->] (Z2) edge node [above, near start,  font=\scriptsize]{$\beta_{1,2}$} (y1);
    \path[->] (Z1) edge node [above, near start,  font=\scriptsize]{$\beta_{2,1}$} (y2);
    \path[->] (Z2) edge node [above,  near start, font=\scriptsize]{$\beta_{2,2}$} (y2);
    \end{tikzpicture}
    %\caption{neural network diagram}
    %\label{fig:nn_graph}
%\end{figure}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{align*}
    %&\mathbf{x} = [x_1, x_2, x_3]^T\\
    &a_1 = \alpha_{1,1} x_1 + \alpha_{1,2} x_2 + \alpha_{1,3} x_3 \\
    &a_2 = \alpha_{2,1} x_1 + \alpha_{2,2} x_2 + \alpha_{2,3} x_3 \\
    %&a_j = \sum_{i=1}^{3} \alpha_{j, i} \cdot x_i , \,\,  \forall j \in \{1, 2\}\\
    &z_j = \max(0, a_i) ,\,\,  \forall j \in \{1, 2\}\\
    &b_1 = \beta_{1,1} z_1 + \beta_{1,2} z_2 \\
    &b_2 = \beta_{2,1} z_1 + \beta_{2,2} z_2 \\
    %&b_k = \sum_{j=1}^{2} \beta_{k, j} \cdot z_j ,\,\, \forall k \in \{1, 2\}\\
    &\hat{y}_k = \exp(b_k)/(\exp(b_1)+\exp(b_2)) ,\,\,  \forall k \in \{1, 2\}\\
    &\ell = - \sum_{k=1}^2 y_k \log(\hat{y}_k)
\end{align*}
\end{minipage}

\begin{qauthor}
    Zoe Xu; Implement a feed-forward neural network; Construct a computation graph for a neural network, identifying all the given and intermediate quantities that are relevant.
\end{qauthor}

\begin{subparts}


\subpart[2] \textbf{Numerical answer:} 
    Given $\xv = [1, 2, 0]^T$, $\alpha_{j,i} = 1\,\,\forall j,i$, $\beta_{k,j} = 1\,\, \forall k,j$. Compute $b_2$. (You should ignore these numerical values for all subsequent questions.)

    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    $b_2 = 6$
    \end{soln}

\subpart[2] \textbf{Math:} What is the chain of partial derivatives needed by symbolic differentiation to calculate the derivative $\frac{\partial \ell}{\partial\alpha_{j,i}}$ for $i \in \{1, 2, 3\}$ and $j \in \{1, 2\}$?

    Your answer should be in the form: 
    $\frac{\partial \ell}{\partial \alpha_{j,i}} = \frac{\partial ?}{\partial ?} \frac{\partial ?}{\partial ?} \dots$
    Make sure each partial derivative $\frac{\partial ?}{\partial ?}$ in your answer cannot be decomposed further into simpler partial derivatives. You may intersperse summations between the $\frac{\partial ?}{\partial ?}$ terms.

    \begin{tcolorbox}[fit,height=3cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{soln}
        $\frac{\partial \ell}{\partial \alpha_{j,i}} = \sum_{k=1}^2 
        \frac{\partial \ell}{\partial \hat{y}_k} 
        \frac{\partial \hat{y}_k}{\partial b_k}
        \frac{\partial b_k}{\partial z_j} 
        \frac{\partial z_j}{\partial a_j} 
        \frac{\partial a_j}{\partial \alpha_{j,i}}$ \newline \newline
        $\frac{\partial \ell}{\partial \alpha_{j,i}} = \sum_{k=1}^2 
        \frac{\partial \ell}{\partial \hat{y}_k}\sum_{n=1}^2 
        \frac{\partial \hat{y}_k}{\partial b_n} 
        \frac{\partial b_n}{\partial z_j} 
        \frac{\partial z_j}{\partial a_j} 
        \frac{\partial a_j}{\partial \alpha_{j,i}}$
    \end{soln}
    \end{tcolorbox}
    \begin{qauthor}
        Shivi, Instantiate the backpropagation algorithm for a neural network
        (Adapted by Matt)
    \end{qauthor}

\clearpage
\subpart[3] \textbf{Math:} What is the sequence of partial derivatives \emph{stored} by the backpropagation algorithm before it computes \textit{any} of the derivatives $\frac{\partial \ell}{\partial\alpha_{j,i}}$ for $i \in \{1, 2, 3\}$ and $j \in \{1, 2\}$?

    Your answer should be in the form of a list: 
    $[\frac{\partial ?}{\partial ?}, \frac{\partial ?}{\partial ?}, \dots , \frac{\partial ?}{\partial ?}]$, such that each item is stored by backpropagation before all items that appear after it in the list.
    Make sure each partial derivative $\frac{\partial ?}{\partial ?}$ in your answer cannot be decomposed further into simpler partial derivatives. 
    
    \begin{tcolorbox}[fit,height=3cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{soln}
        $[
        \frac{\partial \ell}{\partial \hat{y}_1}, 
        \frac{\partial \ell}{\partial \hat{y}_2}, 
        \frac{\partial \ell}{\partial b_1},
        \frac{\partial \ell}{\partial b_2},
        \frac{\partial \ell}{\partial z_1}, 
        \frac{\partial \ell}{\partial z_2},
        \frac{\partial \ell}{\partial a_1}, 
        \frac{\partial \ell}{\partial a_2}
        ]
        $
    \end{soln}
    \end{tcolorbox}
    \begin{qauthor}   Matt    \end{qauthor}
    
\subpart[2] \textbf{Math:} Write an expression for how backpropagaion computes $\frac{\partial \ell}{\partial\alpha_{j,i}}$ for $i \in \{1, 2, 3\}$ and $j \in \{1, 2\}$, after the algorithm has stored all the partial derivatives in your list from the previous question.

    Your answer should be in the form: 
    $\frac{\partial \ell}{\partial \alpha_{j,i}} = \frac{\partial ?}{\partial ?} \frac{\partial ?}{\partial ?} \dots$
%Your answer should be a mathematical expression containing only variables introduced in the problem introduction, along with necessary constants, mathematical notation and/or subscripts.
    
    \begin{tcolorbox}[fit,height=3cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    \begin{soln}
        $\frac{\partial \ell}{\partial\alpha_{j,i}} =
        \frac{\partial \ell}{\partial a_j}
        \frac{\partial a_j}{\partial \alpha_{j,i}}
        $
    \end{soln}
    \end{tcolorbox}
    \begin{qauthor}   Matt    \end{qauthor}

    
    
\subpart[3] Complete the stochastic gradient descent implementation below in order to update $\alpha_{j,i}$ and $\beta_{k,j}$. (You may use $\frac{\partial \ell}{\partial\alpha_{j,i}}$ and/or $\frac{\partial \ell}{\partial\beta_{k,j}}$, if needed.)

    \begin{tcolorbox}[height=7.5cm, width=15cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    %\begin{algorithm}
    %\caption{Stochastic Gradient Descent for One-Hidden-Layer Neural Network}
    \begin{algorithmic}
    \State Initialize weights \(\alpha_{j,i}\) and \(\beta_{k,j}\) randomly
    \State Choose learning rate \(\eta\)
    \For{each epoch}
        \For{each training example \((\xv, \yv)\)}
            \State // Compute gradients
            \State 
            \State 
            \State
            \State
            \State // Update weights
            \State 
            \State 
            \State
            \State
        \EndFor
    \EndFor
    \end{algorithmic}
    %\end{algorithm}
    \end{tcolorbox}
    
    \begin{soln}
    %\begin{algorithm}
    %\caption{Stochastic Gradient Descent for One-Hidden-Layer Neural Network}
    \begin{algorithmic}
    \State Initialize weights \(\alpha_{j,i}\) and \(\beta_{k,j}\) randomly
    \State Choose learning rate \(\eta\)
    \For{each epoch}
        \For{each training example \((\xv, \yv)\)}
            \State // Compute gradients
            \State $g_{\beta_{k,j}} = \frac{\partial \ell}{\partial\beta_{k,j}}, \forall k,j $
            \State $g_{\alpha_{j,i}} = \frac{\partial \ell}{\partial\alpha_{j,i}}, \forall i,j $
            \State // Update weights
            \State \(\beta_{k,j} \gets \beta_{k,j} - \eta  g_{\beta_{k,j}}, \forall k,j \)
            \State  \(\alpha_{j,i} \gets \alpha_{j,i} - \eta g_{\alpha_{j,i}}, \forall i,j \)
        \EndFor
    \EndFor
    \end{algorithmic}
    %\end{algorithm}
    \end{soln}
    \begin{qauthor}   Shivi (Adapted by Matt)    \end{qauthor}
    
        
\subpart[2] Yay! You just finished training your network using stochastic gradient descent. But, Neural the Narwhal tests it out, tells you that it doesn't classify well enough yet, and suggests you add 3 more neurons to the hidden layer. He also suggests prepending a bias term to your input, $\xv$.
    
    With these updates to your network architecture, what are the new dimensions of the weights matrix, $\alphav$? Express your answer as $\alphav \in \mathbb{R}^{r \times c}$, where $r$ is the number of rows and $c$ is the number of columns.
    
    \begin{tcolorbox}[fit,height=1cm, width=6cm, blank, borderline={1pt}{-2pt},nobeforeafter]
     \begin{soln}
    $\alphav \in \mathbb{R}^{5 \times 4}$
    \end{soln}
    \end{tcolorbox}

    \begin{qauthor}
        Shivi, Instantiate the backpropagation algorithm for a neural network
    \end{qauthor}

    
    % \subpart[3] \textbf{Math:} Suppose that $\mathbf{\beta} = \begin{bmatrix} 0.8 \\ 0.5\end{bmatrix}$. Given that $\ell(y, \hat{y}) = \frac{1}{2}(y-\hat{y})^2$, the learning rate $\gamma = 0.01$, and the following values (if you need them), what is $\mathbf{\beta}$ after one SGD update? 

    % $y = 0.5 \newline
    % \hat{y} = 0.8 \newline
    % g(n_1) = 0.7 \newline
    % g(n_2) = 0.5$

    % The elements of $\beta$ should all be numerical values.

    % \begin{tcolorbox}[fit,height=4cm, width=10cm, blank, borderline={1pt}{-2pt},nobeforeafter]
    % \begin{soln}
    %     $\beta_i \leftarrow \beta_i - \gamma*\frac{\partial \ell}{\partial \beta_i}$ \newline
    %     $\frac{\partial \ell}{\partial \beta_i} = \frac{\partial \ell}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \beta_i}$ \newline
    %     $\frac{\partial \ell}{\partial \beta_i} = (\hat{y}-y) * \frac{\partial \hat{y}}{\partial \beta_i}$ \newline
    %     $\frac{\partial \ell}{\partial \beta_1} = 0.3 * 0.7 = 0.21$ \newline
    %     $\frac{\partial \ell}{\partial \beta_2} = 0.3 * 0.5 = 0.15$ \newline
    %     $\beta = \begin{bmatrix} 0.8-(0.01*0.21) \\ 0.5-(0.01*0.15) \end{bmatrix}$ \newline
    %     $\beta = \begin{bmatrix} 0.7979 \\ 0.4985 \end{bmatrix}$
    % \end{soln}
    % \end{tcolorbox}

\subpart[1] \textbf{True or False:} If we switch all nonlinear functions in the given neural network to the identity function, then the resulting neural network will behave identically to a pair of linear regression models.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True
    \end{soln}
    \begin{qauthor}
    Zoe Xu (adapted by Matt)
    \end{qauthor}

\end{subparts}

\part[2] \textbf{Short answer:} Describe three differences between a neural network diagram and a computation graph diagram.    
    \begin{tcolorbox}[height=5cm, width=15cm, blank, borderline={1pt}{-2pt}]
    %solution
    1. \\ \\ \\ 
    2. \\ \\ \\ 
    3.
    \end{tcolorbox}
    \begin{soln}
    TODO
    \end{soln}
    \begin{qauthor} Matt (should really make this a list of characteristics where they select which type of diagram it is true of)
    \end{qauthor}
    





\end{parts}