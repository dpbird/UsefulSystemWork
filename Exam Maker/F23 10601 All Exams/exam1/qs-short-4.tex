% These problems were removed to shorten the total exam time.
\subsection{Decision Trees}
\begin{enumerate}
% WAS 1.4
    \item\pts{4} We learn a decision tree for $N$ training examples $\Dc = \{ (\xv^{(i)}, y^{(i)}) \}_{i=1}^N$ where each feature vector $\xv^{(i)}$ has $M$ attributes. Assume our splitting criteria is mutual information.
    
    \begin{enumerate}[label=\roman*)]
        \item Now suppose we train a new decision tree on an augmented dataset $\Dc'$ to which we add one additional attribute $m'$ that is a copy of attribute $m$. That is $x_{m'}^{(i)} = x_{m}^{(i)}$ for all $\xv^{(i)}$ in $\Dc'$. Does the training error rate of this new decision tree differ from the original one trained on $\Dc$? \textbf{Justify your answer.}
        \fillwithlines{10em}
        
        \begin{soln}
        No, the two decision trees are effectively the same. Since $m$ and $m'$ are the same attribute $m'$ cannot add any information that is not already used in attribute $m$. 
        \end{soln}
        
        \begin{qauthor}
        Bowei- Taken from 10701 Spring 2009. Learning objective 1 and 5
        \end{qauthor}
        
        \item Now suppose we train a new decision tree on an augmented dataset $\Dc''$ to which we add one additional training example $\xv^{(N+1)}$ that is a copy of the training example $\xv^{(N)}$. Does the tree learned on $\Dc''$ differ from the original one learned on $\Dc$? \textbf{Justify your answer.}
        \fillwithlines{10em}
        
        \begin{soln}
        Yes, the decision tree can change. The mutual information in each split depends on the set of samples and copying a vector twice may change the distribution leading to a selection of a different attribute to split on. 
        \end{soln}
        
        \begin{qauthor}
        Bowei- Taken from 10701 Spring 2009. Learning objective 1 and 5
        \end{qauthor}
    
    \end{enumerate}
    
\end{enumerate}

\subsection{Perceptron}
\begin{enumerate}
    
    \item\pts{1} \textbf{True or False:} The perceptron algorithm always finds the separating hyperplane with the largest margin.
    
    \begin{enumerate}
        \item True
        \item False
    \end{enumerate}
    
    \begin{soln}
    False.
    \end{soln}
    
    \begin{qauthor}
        Jennifer    
    \end{qauthor}
    
\end{enumerate}

\subsection{Optimization}

\begin{enumerate}
    
    \item\pts{1} Which of the following has the slowest asymptotic convergence? \textbf{Select one.}
    \begin{enumerate}
        \item Newton's Method
        \item (Batch) Gradient Descent
        \item Stochastic Gradient Descent
    \end{enumerate}
    
    \begin{soln}
    SGD has the slowest rate of convergence
    \end{soln}
    
    \begin{qauthor}
    Elaine Liu, ML as optimization 2a and 2b (inspired by lecture on optimization)
    \end{qauthor}
    
\end{enumerate}

\subsection{Neural Nets}

\begin{enumerate}

    \item\pts{4} Which of the following is \textbf{true} about neural networks?
    \textbf{Select all that apply.}
    \begin{enumerate}
        \item We can use a 1 hidden-layer neural network with logistic activation functions to approximate any continuous function as long as there are enough hidden units.
        \item One of the advantage of neural networks is that training always converges to the global minimum  of the objective if the step size is small enough.
        \item Neural networks require less feature engineering because they can learn the features from data automatically.
        \item A neural network can use linear activation functions at every layer to learn any nonlinear decision boundary as long as there are enough hidden layers. 
        \item None of the above.
    \end{enumerate}
    
    \begin{soln}
    \begin{enumerate}
        \item is correct because neural nets are universal function approximators.
        \item is incorrect because neural networks can only converge to a local minimum.
        \item is correct.        
        \item is incorrect because if there is no nonlinear activation function the decision boundary would always be linear.
    \end{enumerate}
    \end{soln}
    
    \begin{qauthor}
    Shawn Lyu
    \end{qauthor}

\end{enumerate}