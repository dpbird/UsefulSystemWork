\sectionquestion{F22 TA Questions Go Here!}

\begin{parts}



   

\begin{comment}
    \part[2] \textbf{Short answer:} Suppose we have a very large dataset $D$ and a model type (e.g. perceptron). Let's define "K-fold error distribution" as the result of the following procedure:
    \begin{enumerate}
        \item Repeat many, many times:
        \begin{enumerate}
            \item Randomly select 1000 samples from $D$.
            \item Perform K-fold cross-validation on that set of samples.
        \end{enumerate}
        \item Return the distribution of cross-validation error rates.
    \end{enumerate}
    Compare the mean and variance of the 10-fold error distribution with those of the 1000-fold error distribution.
        \fillwithlines{2em}
        \fillwithlines{2em}
        \fillwithlines{2em}
        \fillwithlines{2em}
        \begin{soln}
        Due to law of large numbers, the means of both error distributions will likely be the same. However, the 1000-fold error distribution will have a much larger variance than the 10-fold error distribution since 1000-fold cross-validation uses just a single data point for testing (which could be an outlier). See bias-variance tradeoff.
        \end{soln}
        \begin{qauthor}
        Author: Brandon Wang
        Objective: Plan an experiment that uses training, validation, and test datasets to predict the performance of a classifier on unseen data (without cheating)
        
        Removed by Henry: somewhat out-of-scope for exam 1
        \end{qauthor}
    
    \newpage
    \part Below we are given a graph, and we can construct a decision tree based on these types of splits: $y > n$, $y < n$, $x > n$, $x < n$, where $n$ is some number that we choose. 

    \begin{figure}[H]
    \centering
    \begin{tikzpicture}
    \begin{axis}[
        scale=1.0, width=15cm, height=9cm,
        xmin=0, xmax=7, xtick={1,2,3,4,5,6},
        ymin=0, ymax=6, ytick={1,2,3,4,5},
        samples=50, xlabel=$x$, ylabel=$y$]]
        %\addplot[blue, ultra thick] (x,x*x);
        %\addplot[red,  ultra thick] (x*x,x);
        \addplot [
            scatter,
            only marks,
            point meta=explicit symbolic,
            scatter/classes={
                a={mark=x,red,scale=2,ultra thick},
                b={mark=o,blue,scale=2,ultra thick}
            },
            nodes near coords*={},
            visualization depends on={\thisrow{myvalue} \as \myvalue},
        ] table [meta=label] {
            x y label myvalue
            1 3 b 1
            3 2 b 1
            7 3 b 1
            0 5 a 1
            4 5 a 1
            5 4 a 1
            5 2 a 1
        };
    \end{axis}
    \end{tikzpicture}   
  \caption{ }
  \label{fig:dt2ddata}
\end{figure}

    \begin{subparts}
    
    \subpart[2] What's the minimum error rate that we can achieve using a decision tree of depth 1?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
        %solution
    \end{tcolorbox}

    \subpart[2] What's the minimum possible error rate that we can achieve using a decision tree of any depth?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
        %solution
    \end{tcolorbox}

    \newpage
    \subpart[3] Draw the minimum possible depth decision tree that can get us the minimum possible error rate.
    \begin{tcolorbox}[fit,height=15cm, width=15cm, blank, borderline={1pt}{-2pt}]
        %solution
    \end{tcolorbox}
    \end{subparts}
\begin{qauthor}
    Removed by Henry for overlap with other questions
\end{qauthor}
\begin{qtester}
    EA Feedback: No author or solutions given. I think we should explain what x,y,and n are in this example.
\end{qtester}

    \newpage
    \part[2] \textbf{Select one:} Which of the following statements is true:
        \begin{checkboxes}
            \choice The test set must be untouched during model training and tuning in order to obtain an independent, true evaluation of the model’s hypothesis.
            \choice The validation set must be untouched during model training and tuning in order to obtain an independent, true evaluation of the model’s hypothesis.
            \choice The validation set may be used for building the model since it contains held-out data that can give a good estimate of model performance.
            \choice The training set may be used to augment the test set if there aren’t enough datapoints in the test set.
        \end{checkboxes}
        \begin{soln}
        A;
        A is the correct answer - the test set must remain untouched during training and tuning.
        B is incorrect because the validation set is used during model tuning, C is incorrect since the validation set must not be used while training the model, D is incorrect because datapoints from the training set must not be included in the test set.
        \end{soln}
        \begin{qauthor}
        Author: Aditi Sharma
        Objective: Explain the difference between (1) training error, (2) validation error, (3) cross-validation error, (4) test error, and (5) true error
    
        Removed by Henry for ambiguous wording
        \end{qauthor}

        \begin{qtester}
        EA Feedback: I think this is a good question. 
        \end{qtester}
\end{comment}


    \newpage




% \part Let's consider binary operators as target functions. For this question, use the version of the binary operator that takes in and returns a single bit, that is, the labels are either 0 or 1.
%     \begin{subparts}
%     \subpart[1] \textbf{Select all that apply:} Which of the following functions yield points that \textbf{cannot} be classified correctly by a linear decision boundary?
%     {%
%     \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
%     \begin{checkboxes}
%      \choice AND
%      \choice OR
%      \choice XOR
%      \choice None of the above
%     \end{checkboxes}
%     }
%     \begin{soln}
%     XOR(0,0)=0=-
%     XOR(1,0)=1=+
%     XOR(0,1)=1=+
%     XOR(1,1)=0=-

%     XOR cannot be classified by a linear decision boundary.
    
%     \end{soln}

%     \addpoints
%     \subpart[1]\textbf{Short answer:} If you believe any of the functions in the previous question yield points that could not be linearly separable, what models that we have learned in class could be used to correctly classify those points?
%     \fillwithlines{2em}
%     \begin{soln}
%     1-nearest neighbor; 2-nearest neighbor; decision tree
%     \end{soln}


    
%     \end{subparts}

\begin{comment}
\part[1] \textbf{Short answer:} Let's consider binary operators as target functions. For this question, use the version of the binary operator that takes in and returns a single bit, that is, the labels are either 0 or 1. \textbf{Identify} 1 model that could correctly classify the XOR function. 
\fillwithlines{2em}
    \begin{soln}
    1-nearest neighbor; 2-nearest neighbor; decision tree; perceptron
    \end{soln}

    \begin{qauthor}
    Kalvin Chang\\
    Objective: Identify whether a dataset is linearly separable or not\\
    Source: CIML 4.7\\
    (Note: I just realized that this may be too similar to a question on HW3)
    
    Removed by Henry for overlap with HW3 question
    \end{qauthor}

    \begin{qtester}
    I think that part b requires that part a not have option D be the correct answer. I wonder if this question would be more effective rolled into one question?
    \end{qtester}
\end{comment}




    
    \newpage

\begin{comment}
\part[2] \textbf{Select one:} Which of the following statements is not a cause of overfitting in decision trees:
        \begin{checkboxes}
            \choice The presence of noise in the instance labels.
            \choice The tree perfectly fits all of the training samples.
            \choice The lack of representative instances.
            \choice The testing error is low.
        \end{checkboxes}
        \begin{soln}
        D is the correct answer - Overfitting causes testing error to be high.
        \end{soln}
        
        \begin{qauthor}
        Author: Samiksha Kale
        Objective: Judge whether a decision tree is "underfitting" or "overfitting"
        
        Removed by Henry for ambiguous language
        \end{qauthor}

        \begin{qtester}
        EA Feedback: Nice question!
        \end{qtester}

\part[2] \textbf{Select one:} What is the correct bias variance tradeoff for a decision tree that is underfitting:
        \begin{checkboxes}
            \choice low variance, high bias.
            \choice low variance, low bias.
            \choice high variance, high bias.
            \choice high variance, low bias.
        \end{checkboxes}
        \begin{soln}
        A is the correct answer - High bias means predictions are almost always off and the low variance comes from simplistic models both of which lead to  underfitting
        \end{soln}
        
        \begin{qauthor}
        Author: Samiksha Kale
        Objective: Judge whether a decision tree is "underfitting" or "overfitting"
        
        Removed by Henry: out of scope for exam 1
        \end{qauthor}
\end{comment}



        



\begin{comment}
\part[2] \textbf{Short answer:} Neural is doing hyperparameter tuning when he decides that, instead of choosing a hyperparameter based on validation error, he'll choose his hyperparameter based on the training error. Explain why this is not a good idea.

\fillwithlines{8em}

\begin{soln}
We want to pick hyperparameters that minimize generalization error; picking the hyperparameters that minimize error on the training set may cause us to choose a granular model that doesn't necessarily generalize the best.
\end{soln}
\begin{qauthor}
Chu. Mostly focusing on decision trees and k-NN. Model selection. This question might be kinda hard as-is; I think making it specifically about tuning $k$ for k-NN or max-depth for decision trees could make it easier.

Removed for time by Henry
\end{qauthor}

\begin{qtester}
Maybe change this to "Is this a good idea? Explain why or why not."

Great questions, Chu!
\end{qtester}

\end{comment}


\end{parts}