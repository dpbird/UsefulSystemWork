\section{TAs should input their questions here (short.tex)}

\section{Decision Trees}
\begin{questions}
\question Decision trees in two scenarios: large sample, let student work with distributions of data rather than a toy example. Then we get back to a small sample case and discuss what is the best way to handle unseen attribute values (random guess v.s. majority vote at this internal node).

Note: carrying out this question requires a significant amount of work so I will first leave a plan here and once it's approved I will start fleshing it out.

In the \textbf{large sample} scenario, we provide the full data distribution: 3 binary attributes and binary label. For each of the 8 possible sample values, what's their probability and for each sample, what's the distribution of label y. Questions:

\begin{enumerate} [label=(\roman*)]
    \item What is the best possible error rate given this data?
    \item Run ID-3 on this data distribution, until it's perfectly classified.
    \item True or False: under infinite amount of data coming from the true data distribution, decision tree will still overfit if we do not limit the depth.
\end{enumerate}

In the \textbf{small sample} scenario, we give 10 samples from the distribution. The samples are handpicked such that: a) not all 8 attribute values are present; b) some values will not have the same majority vote result as the true distribution; Questions:

\begin{enumerate} [label=(\roman*)]
    \item Run ID-3 on this small sample until perfectly classified.
    \item What's the population error rate if we randomly guess for attribute values that are not in one of the tree branches. 
    \item What's the population error rate if, for attribute values that are not in one of the tree branches, we predict the majority vote result in an internal node. Is it better?
    \item Is the tree overfitting, why? If so, cut one branch to make the population error rate lower (use the "internal node's majority vote" policy).
\end{enumerate}


\begin{qauthor}
    (1) By Sida, (2) Learning Objectives: "Implement Decision Tree training and prediction", "generalization", "mutual information / information gain", "difference between true error and training error", "overfitting", "pruning method to combat overfitting" (3) Many regrade requests are on one questionable way to handle unseen feature values in the training set, so I came up with this question to have a further discussion.
    \end{qauthor}

\question[]\textbf{Perceptron Trees: } To exploit the desirable properties of decision tree classifiers and perceptrons, Adam came up with a new algorithm called ``perceptron trees'', which combines features from both. Perceptron trees are similar to decision trees, however each leaf node is a perceptron, instead of a majority vote. 

To create a perceptron tree, the first step is to follow a regular decision tree learning algorithm (such as ID3) and perform splitting on attributes until the specified maximum depth is reached. Once maximum depth has been reached, at each leaf node, a perceptron is trained on the remaining attributes which have not been used up in that branch. Classification of a new example is done via a similar procedure. The example is first passed through the decision tree based on its attribute values. When it reaches a leaf node, the final prediction is made by running the corresponding perceptron at that node.

Assume that you have a dataset with 6 binary attributes \textbf{(A, B, C, D, E, F)} and two output labels \textbf{(-1 and 1)}. A perceptron tree of depth 2 on this dataset is given below (assume bias=1 for each perceptron):

\begin{figure}[H]
        \centering
        \includegraphics[width = 0.7\textwidth]{fig/ptree.png}
        \caption{Perceptron Tree of max depth=2}
        \label{L1_norm}
\end{figure}

    \noaddpoints % to omit double points count
    \begin{parts}
    \part[] \textbf{Numerical answer:} Given a sample $\x = [1,1,0,1,0,1]$, predict the output label for this sample 
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    1, Explanation: A=1 and D=1 so the point is sent to the right-most leaf node, where the perceptron output is (1*1)+(0*0)+((-1)*0)+(1*1)+1 = 3. Prediction = sign(3) = 1.
    \end{soln}
    \part[] {\bf True or False:} The decision boundary of a perceptron tree will \textit{always} be linear.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False, since decision tree boundaries need not be linear.
    \end{soln}
    \part[] {\bf True or False:} For small values of max depth, decision trees are\textit{ more }likely to underfit the data than perceptron trees
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True. For smaller values of max depth, decision trees essentially degenerate into majority-vote classifiers at the leaves. On the other hand, perceptron trees have the capacity to make use of ``unused'' attributes at the leaves to predict the correct class.
    \end{soln}
    \part[] \textbf{Short answer:} Name one desirable feature that perceptron trees inherit from decision trees and perceptron each
    \fillwithlines{10em}
    \begin{soln}
    Decision trees: Non-linear decision boundaries\\
    Perceptron: Ability to gracefully handle unseen attribute values in training data/ Better generalization at leaf nodes
    \end{soln}
    \end{parts}
    \addpoints
    
\begin{qauthor}
    (1) Aakanksha (2) Learning objectives addressed: Implement prediction in decision tree and perceptron, Decision boundaries for both models (3) Initially inspired by a recent paper: \url{https://www.transacl.org/ojs/index.php/tacl/article/view/1218/282}, came across this 2008 exam question later: \url{http://mas.cs.umass.edu/classes/cs683/exams/final_2008.pdf}
\end{qauthor}

\end{questions}

\section{KNN}
\begin{questions}
\question[] \textbf{True or False:} Consider a binary (two classes) classification problem using k-nearest neighbors. We have n 1-dimensional training points $\{x_1,x_2,...,x_n\}$ with $x_i \in \Rb$, and their corresponding labels $\{y_1, y_2, ..., y_n\}$ with $y_i \in \{0,1\}$. 

Assume the data points $x_1,x_2,...,x_n$ are sorted in the ascending order, we use Euclidean distance as the distance metric, and a point can be it's own neighbor. True or False: We \textbf{CAN} build a decision tree (with decisions at each node has the form ``$x \geq t$" and ``$x < t$", for $t \in \Rb$) that behave exactly the same as the 1-nearest neighbor classifier, on this dataset.

    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True, we can build a decision tree by setting the internal nodes at the mid-points between each pair of adjacent training points.
    \end{soln}
    \begin{qauthor}
    (1) By George, (2) Sketch the decision boundary for a learning algorithm; 
    Implement Decision Tree training and prediction, (3) source if  adapting/reusing a question: HW1 from F17 10-701
    \end{qauthor}

\question [] \textbf{Select all that apply:} Please select all that apply about kNN in the following options: 

Assume a point can be its own neighbor.

    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice k-NN works great with a small amount of data, but struggles when the amount of data becomes large. 
     \choice k-NN is sensitive to outliers; therefore, in general we decrease k to avoid overfitting.
     \choice k-NN can only be applied to classification problems, but it cannot be used to solve regression problems.
     \choice We can always achieve zero training error (perfect classification) with k-NN, but it may not generalize well in testing.
    \end{checkboxes}
    }
    \begin{soln}
    True: A, Curse of dimensionality; D, by setting k = 1\\
    False: B, we increase k to avoid overfitting; C, KNN regression\\
    Analysis:
    \end{soln}
    \begin{qauthor}
    (1) By George
    (2) learning objective addressed: curse of dimensionality; understand the difference between training error and testing error; overfitting and underfitting
    (3) No source.
    \end{qauthor}

\end{questions}

\section{Perceptron}
\begin{questions}
\question {\bf Select all that apply:} Let $S = \{(\mathbf{x}^{(1)},y^{(1)}),\cdots,(\mathbf{x}^{(n)},y^{(n)})\}$ be $n$ linearly separable points by a separator through the origin in $\mathbb{R}^d$. Let $S^\prime$ be generated from $S$ as: $S^\prime = \{(c\mathbf{x}^{(1)},y^{(1)}),\cdots,(c\mathbf{x}^{(n)},y^{(n)})\}$, where $c > 1$ is a constant. Suppose that we would like to run the perceptron algorithm on both data sets separately, and that the perceptron algorithm converges on $S$. Which of the following statements are true?

    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice The mistake bound of perceptron on $S^\prime$ is larger than the mistake bound on $S$
     \choice The perceptron algorithm when run on $S$ and $S^\prime$ returns the same classifier, modulo constant factors (i.e., if $\mathbf{w}_S$ and $\mathbf{w}_{S^\prime}$ are outputs of the perceptron for $S$ and $S^\prime$, then $\mathbf{w}_S = c_1 \mathbf{w}_S^\prime$ for some constant $c_1$).
     \choice The perceptron algorithm converges on $S^\prime$.
    \end{checkboxes}
    }
    \begin{soln}
    B and C are true.. Simply follow the perceptron update rule and we see that the update on $\mathbf{w}_S$ and $\mathbf{w}_{S^\prime}$ is identical up to the constant $c$. A is false as the maximum margin between any point to the decision hyperplane is also scaled up by $c$, and the mistake bound is unchanged.
    \end{soln}
    
    \begin{qauthor}
    (1) By Oliver
    (2) learning objective addressed: perceptron mistake bound; perceptron update rule.
    (3) No source.
    \end{qauthor}
    
\question [] {\bf True or False:} We know that if the samples are linearly separable, the perceptron algorithm finds a separating hyperplane in a finite number of steps. Given such a dataset with linearly separable samples, select whether the following statement is True or False:\\
The running time of the perceptron algorithm depends on the sample size n.
    \begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False. For a linearly separable dataset, the runtime of the perceptron algorithm does not depend on the size of the training data. The proof can be found on slide 34 of  \url{http://www.cs.cmu.edu/~10701/slides/8_Perceptron.pdf}
    \end{soln}
    
    \begin{qauthor}
    (1) By Varsha Chinnaobireddy
    (2) learning objective addressed: perceptron running time 
    (3) No source.
    \end{qauthor}
\end{questions}

\section{Linear Regression}
\begin{questions}
\question [] \textbf{Short answer:} Assume we have data $\mathbf{X} \in \mathbb{R}^{n \times d}$ with label $\mathbf{y} \in \mathbb{R}^n$. If the underlying distribution of the data is $\mathbf{y} = \mathbf{X} \mathbf{\beta}^* + \mathbf{\epsilon}$, where $\mathbf{\epsilon} \sim N(0, \mathbf{I})$. Assume the closed form solution $\hat{\beta}$ for mean squared error linear regression exists for this data, write out $\hat{\beta}$'s distribution:
    \fillwithlines{2em}
    \begin{soln}
    We first write out the closed form solution, the we plug in $\mathbf{y} = \mathbf{X} \mathbf{\beta}^* + \mathbf{\epsilon}$. If the question is approved I will flesh out the (3-line-ish) solution.
    
    \end{soln}
    \begin{qauthor}
    Input (1) Sida, (2) learning objective addressed: "closed form for linear regression", and (3) Came up in an interview.
    \end{qauthor}


\question [] Consider linear regression on 1-dimensional data $\mathbf{x} \in \mathbb{R}^{n}$ with label $\mathbf{y} \in \mathbb{R}^n$. We apply linear regression in both directions on this data, i.e., we first fit $y$ with $x$ and get $y = \beta_1 x$ as the fitted line, then we fit $x$ with $y$ and get $x = \beta_2 y$ as the fitted line. Discuss the relations between $\beta_1$ and $\beta_2$:

\begin{enumerate} [label=(\roman*)]
\item \textbf{True or False:} The two fitted lines are always the same, i.e. we always have $\beta_2 = \frac{1}{\beta_1}$.

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False.
    \end{soln}

\item \textbf{Numerical answer:} We further assume that $\mathbf{x}^T\mathbf{y} > 0$. What is the minimum value of $\frac{1}{\beta_1} + \frac{1}{\beta_2}$?
    \begin{tcolorbox}[fit,height=1cm, width=2cm, blank, borderline={1pt}{-2pt}]
    %solution
    \end{tcolorbox}
    \begin{soln}
    2.
    \end{soln}

\end{enumerate}
    \begin{qauthor}
    Input (1) Sida, (2) learning objective addressed: "closed form for linear regression", and (3) Linear regression is oddly popular in interviews.
    \end{qauthor}


\end{questions}

\section{Optimization}
\begin{questions}
\question [] \textbf{Select all that apply:} Which of the follows are correct regarding Gradient Descent (GD). Assume data log-likelihood is $L(\theta|X)$, which is a function of the parameter $\theta$, and the objective function is negative log-likelihood .
    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice GD requires that $L(\theta|X)$ is concave with respect to parameter $\theta$ in order to converge
     \choice GD requires that $L(\theta|X)$ is convex with respect to parameter $\theta$ in order to converge
     \choice GD update rule is $\theta \leftarrow \theta - \alpha \frac{ \partial L(\theta|X)}{\partial \theta}$
     \choice Given a fixed small learning rate (say $ \alpha =10^{-10}$), GD will always reach the optimum after infinite iterations (assume that the objective function satisfies the convergence condition).
    \end{checkboxes}
    }
    \begin{soln}
    A
    
    Analysis:
    C should replace minus with plus. D is wrong because it is possible that $\theta$ will  jump around the minimum and never reach the optimum even though $\alpha$ is  (finitely) small.
    \end{soln}
    \begin{qauthor}
    (1) By Rongye 
    (2) learning objective addressed: Distinguish between convex, concave, and nonconvex functions; Apply gradient descent to optimize a function 
    (3) No source.
    \end{qauthor}

\question [] \textbf{Select all that apply:} Which of the following are correct regarding Gradient Descent (GD) and stochastic gradient descent (SGD)
    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice Each update step in SGD pushes the parameter vector closer to the parameter vector that minimizes the objective function.
     \choice The gradient computed in SGD is, in expectation, equal to the gradient computed in GD.
     \choice The gradient computed in GD has a higher variance than that computed in SGD, which is why in practice SGD converges faster in time than GD.
     \choice Both SGD and GD are gauranteed to find a solution if and only if the objective function is strongly convex. That is if the following is true for the objective function $f$ and points $x_1, x_2$ where $x_1 \neq x_2$:
     \[
        f(tx_1 + (1 - t)x_2) < tf(x) + (1-t)f(x_2)\qquad 0\leq t \leq 1
     \]
    \end{checkboxes}
    }
    \begin{soln}
    B.
    
    A is incorrect, SGD updates are high in variance and may not go in the direction of the true gradient. C is incorrect, for the same reason. D is incorrect since they can converge if the function is convex, not just strongly convex.
    \end{soln}
    \begin{qauthor}
    (1) Emilio, (2) Distinguish between convex, concave, and nonconvex functions, Apply stochastic gradient descent (SGD) to optimize a function, Apply gradient descent to optimize a function (3) source: none
    \end{qauthor}

\question [] Let $X_1,X_2,...,X_N$ be i.i.d. data from a uniform distribution over a diamond-shaped area with edge length $\sqrt{2}\theta$ in $\Rb^2$, where $\theta \in \Rb^+$ (see Figure \ref{L1_norm}). Thus, $X_i \in \Rb^2$ and the distribution is 
$$
p(x|\theta )=\left\{\begin{matrix}
 \frac{1}{2\theta ^2}& if \left \| x \right \|\leq \theta \\ 
 0&  otherwise
\end{matrix}\right.
$$
where $ \left \| x \right \| = |x_1|+|x_2|$ is $L1$ norm. Please find the maximum likelihood estimator of $\theta$.

\begin{figure}[H]
        \centering
        \includegraphics[width = 0.2\textwidth]{fig/L1_norm.png}
        \caption{Area of $\left \| x \right \|\leq \theta$}
        \label{L1_norm}
\end{figure}

    \begin{soln}
    Analysis:
    
    The likelihood function is
    
    $$
    L(X_1,X_2,...,X_N;\theta )=\frac{1}{(2\theta ^2)^N} 1\left \{\underset{1\leq i\leq N}{max} \left \| X_i \right \|\leq \theta \right \}
    $$
    To maximize likelihood, we want $\theta$ to be as small as possible with the constraint of $\underset{1\leq i\leq N}{max} \left \| X_i \right \|\leq \theta $, otherwise the likelihood drops to 0. So the MLE of $\theta$ is 
    $$
    \widehat{\theta}= \underset{1\leq i\leq N}{max} \left \| X_i \right \|
    $$
    
    \end{soln}
    \begin{qauthor}
    (1) By Rongye 
    (2) learning objective addressed: Apply the principle of maximum likelihood estimation (MLE) to learn the parameters of a probabilistic model 
    (3) Changed from 10-701 2015 fall midterm exam problem 1.2.
    \end{qauthor}

    \question [] \textbf{Short answer:} Suppose we want to model a 1-dimensional dataset of $N$ real valued features $\left(x^{(i)}\right)$ and targets $\left(y^{(i)}\right)$ by:
    \[
        y^{(i)} \sim \mathcal{N}\left(\exp(wx^{(i)}), 1\right)
    \]
    Where $w$ is our unknown (scalar) parameter and $\mathcal{N}$ is the normal distribution with probability density function:
    \[
    f(a)_{\mathcal{N}(\mu, \sigma^2)} = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{(a - \mu)^2}{2\sigma^2}\right)
    \]
    Can the maximum conditional negative log likelihood estimator of $w$ be solved analytically? If so, find the expression for $w_\text{MLE}$. If not, say so and write down the update rule for $w$ in gradient descent.
    \fillwithlines{2em}
    \begin{soln}
    Cannot be found analytically. 
    
    Taking the derivative of the negative log likelihood with respect to $w$ yields:
    \[
    \frac{\partial \text{NLL}}{\partial w} = \frac{\sum_{i}^N -2x^{(i)}y^{(i)}\exp(wx^{(i)}) + x^{(i)}\exp(2wx^{(i)})}{2}
    \]
    Update rule is thus
    \[
    w \leftarrow w - \eta \frac{\partial \text{NLL}}{\partial w}
    \]
    \end{soln}
    \begin{qauthor}
    (1) Emilio (2) Apply the principle of maximum likelihood estimation (MLE) to learn the parameters of a probabilistic model, Apply knowledge of zero derivatives to identify a closed-form solution (if one exists) to an optimization problem, Apply gradient descent to optimize a function (3) Adapted from 10701 Fall 2003 Midterm (problem setup)
    \end{qauthor}

\end{questions}


\section{Logistic Regression}
\begin{questions}
\question [] \textbf{Select one:} By appropriately transforming features, binary logistic regression model can solve non-linear classification problems. Please select one of the following models that can generate the necessary boundary (it crosses the origin) and solve the binary classification problem in figure \ref{nonlinear_lr}. (Define symbols "+" and "-" as label 1 and label 0, respectively. $x = [x_1,x_2]^T \in \Rb^2, y \in \{0,1\}, \theta_i \in \Rb$.)

\begin{figure}[H]
        \centering
        \includegraphics[width = 0.3\textwidth]{fig/LR_boundary.png}
        \caption{Non-linear classification boundary}
        \label{nonlinear_lr}
\end{figure}

    {%
    \begin{checkboxes}
     \choice $p(y=1|x,\theta)=\frac{1}{1+e^{-(\theta_1 x_1+\theta_2 x_2+\theta_3)}}$
     \choice $p(y=1|x,\theta)=\frac{1}{1+e^{-(\theta_1 x_1^2+\theta_2 x_2^2+\theta_3 x_1 +\theta_4 x_2 + \theta_5)}}$
     \choice $p(y=1|x,\theta)=\frac{1}{1+e^{-(\theta_1 x_1^3 + \theta_2 x_1^2 + \theta_3 x_1 + \theta_4 x_2 + \theta_5)}}$
     \choice $p(y=1|x,\theta)=\frac{1}{1+e^{-(\theta_1 x_1^3+\theta_2 x_2+\theta_3)}}$.
    \end{checkboxes}
    }
    \begin{soln}
    C
    
    Analysis:
    Binary logistic regression $p(y=1|x,\theta)=\frac{1}{1+e^{-\theta^{T}x}}$ is a linear model because the label decision is made by the linear boundary $\theta^T x =0$. Similarly, the model of $p(y=1|x,\theta)=\frac{1}{1+e^{-f(x)}}$ has the boundary of $f(x)=0$. The boundary in the figure is polynomial and has three so-called zero-crossing points ($x_2=0$), say $x_1=$a, 0, and b. So, any point on the boundary satisfies $x_2=c(x_1-a)x_1(x_1-b)= cx_1^3+c(a+b)x_1^2+abcx_1$, i.e., $ f(x)=cx_1^3+c(a+b)x_1^2+abcx_1 -x_2 =0$. Therefore, in order to generate the boundary showed in the figure, the terms $x_1^3,x_1^2,x_1,x_2$ should all show up in $f(x)$. Only C satisfies this requirements.
    \end{soln}
    \begin{qauthor}
    (1) By Rongye 
    (2) learning objective addressed: Understand why the decision boundary of binary logistic regression is
linear; Engineer appropriate features for a new task.
    (3) No source.
    \end{qauthor}


\question 

True/ False: 

- The logistic negative log-likelihood has a global minimum \\
- the logistic negative log-likelihood has a closed-form solution \\
- The log-loss can continue decreasing even after the model achieves perfect training accuracy \\

\question [] Let $D = {(\x^{(i)},y^{(i)}})_{i=1}^n$ be a nonempty probabilistic classification training dataset, $\x^{(i)} \in \Rb^p$ and $y^{(i)} \in \{0,1\}$ for all $i \in \{1,..,n\}$. Let $\w \in \Rb^p$ be a binary logistic regression weight vector(without an explicit bias term) \textbf{that would assign the correct labels to all examples in} $D$. Respond to the following statement with "true" or "false" followed by some justification:

There is a $p \times p$ dimensional matrix $\A$ depending only on $p$ (not $\w$ nor the training set) such that $\A\w$ is guaranteed to assign a strictly greater likelihood to the training data than $\w$ does.
    
    \begin{soln}
    True. Let $A$ = $\frac{1}{k}I$ where k $>$ 1. As $\bold{w}$ correctly classifies all x, $w^T x_i>0$ if $y_i=1$. Only then can $\frac{1}{1+e^{-w^T x_i}}>$ 0.5. $w^T kIx_i>w^Tx_i$. $\frac{1}{1+e^{-w^TkI x_i}}>\frac{1}{1+e^{-w^T x_i}}$ if $y_i=1$. Similarly, $w^T x_i\leq0$ if $y_i=0$. Only then can $\frac{1}{1+e^{-w^T x_i}}\leq$ 0.5. $w^T kIx_i\leq w^Tx_i$. $\frac{1}{1+e^{-w^TkI x_i}}\leq \frac{1}{1+e^{-w^T x_i}}$ if $y_i=0$.
    \end{soln}
    
    \begin{qauthor}
    (1) By Varsha Chinnaobireddy
    (2) learning objective addressed: logistic regression weights
    (3) 10701 Fall 2017 mid-term question
    \end{qauthor}

\question [] \textbf{Select one:} In binary logistic regression, given parameters $\theta$, we use the function $f(x) = \frac{1}{1+e^{-\theta^{T}x}}$ to perform classification. We predict $x$'s label as 1 if $f(x) \geq t$ and as 0 if $f(x) < t$, where $t=0.5$ for this particular choice of $f(x)$. The choice of $f(x)$ is up to us (recall in lecture notes we started with non-differentiable $sign(\theta^Tx)$). Now consider another suitable function $g(x) = \frac{e^{\theta^{T}x} - e^{-\theta^{T}x}}{e^{\theta^{T}x} + e^{-\theta^{T}x}}$ with same $\theta$ for binary classification, which of the following statements is true?  
    {%
    \begin{checkboxes}
     \choice $f(x)$ and $g(x)$ classify $x$ similarly, $t = 0.5$ for $g(x)$
     \choice $f(x)$ and $g(x)$ classify $x$ differently, $t = 0.5$ for $g(x)$
     \choice $f(x)$ and $g(x)$ classify $x$ similarly, $t = 0$ for $g(x)$
     \choice $f(x)$ and $g(x)$ classify $x$ differently, $t = 0$ for $g(x)$
    \end{checkboxes}
    }
    \begin{soln}
    C
    
    Analysis:
    g(x) is tanh($\theta^Tx$), g(x) = 2f(2x)-1. Labelling is similar, threshold t = 0 is obtained by substituting decision boundary $\theta^Tx=0$ in g(x)
    
    \end{soln}
    \begin{qauthor}
    (1) By Rawal 
    (2) learning objective addressed: relationship between threshold, decision boundary and activation function
    (3) No source.
    \end{qauthor}
    
\question [] \textbf{Select one:} On which of the following datasets can a logistic regression classifier $p(y=1|x,\theta)=\frac{1}{1+e^{-\theta^T x}}$, $x = [x_1, x_2]^T$  achieve zero training error? 

\begin{figure}[H]
    \centering
    \begin{adjustbox}{minipage=\linewidth,scale=0.5}
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/lr/data1.png}
        \caption[Dataset 1]%
        {{\small Circle Dataset}}    
        \label{fig:lr_data1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/lr/data2.png}
        \caption[Dataset 2]%
        {{\small Exclusive OR Dataset}}    
        \label{fig:lr_data2}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/lr/data3.png}
        \caption[Dataset 3]%
        {{\small Gaussian Dataset}}    
        \label{fig:lr_data3}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{fig/lr/data4.png}
        \caption[Dataset 4]%
        {{\small Spiral Dataset}}    
        \label{fig:lr_data4}
    \end{subfigure}
    \end{adjustbox}
\end{figure}

    {%
    \begin{checkboxes}
     \choice Circle dataset
     \choice Exclusive OR dataset
     \choice Gaussian dataset
     \choice Spiral dataset
    \end{checkboxes}
    }
    \begin{soln}
    C
    
    Analysis:
    Only C is linearly separable.
    
    \end{soln}
    \begin{qauthor}
    (1) By Rawal 
    (2) learning objective addressed: linear decision boundary of logistic regression 
    (3) No source, this is a standard problem. Datasets modified from playground.tensorflow.org
    \end{qauthor}

\end{questions}

\section{Regularization / Feature Engineering}
\begin{questions}

\question [] Alex was an intern at Pineapple this past summer. One day, Alex was assigned a feature engineering task, the goal of which was to construct dataset $\mathcal{D} = \{\mathbf{X}, \mathbf{y}\}$, where $\mathbf{X} \in \mathbb{R}^{n \times d}$ and $\mathbf{y} \in \mathbb{R}^{n}$ (i.e. the dataset has $n$ samples, each has a $d$-dimensional feature vector and real-valued label). Unfortunately, Alex messed up and included the last feature twice. Formally, instead of getting $\mathbf{X}$, Alex got $\mathbf{X'} \in \mathbb{R}^{n \times (d + 1)}$, where $\mathbf{X'} = [\mathbf{X}, \mathbf{X}_{:, d}]$. Alex made some statements to argue that this mistake was negligible. If you were the manager, how would you respond?

\begin{enumerate} [label=(\roman*)]
\item \textbf{True or False:} The decision tree learned on $\{\mathbf{X'}, \mathbf{y} \}$ will be equivalent to the tree learned on $\{ \mathbf{X}, \mathbf{y} \}$. Two decision trees are equivalent if and only if: a) At every internal node, the attribute being split on is identical, or attribute $d + 1$ in the duplicated date and attribute $d$ in the original data; b) All branch values are the same; c) All leaf labels are identical.

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True.
    \end{soln}

\item \textbf{True or False:} The k-Nearest-Neighbor result on $\{\mathbf{X'}, \mathbf{y} \}$ will be identical to the kNN results on $\{ \mathbf{X}, \mathbf{y} \}$. 

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False.
    \end{soln}

\item \textbf{True or False:} If the closed form solution exists for mean squared error linear regression on $\{\mathbf{X}, \mathbf{y} \}$, then on $\{ \mathbf{X'}, \mathbf{y} \}$ we are also able to find the closed-form solution. 

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False.
    \end{soln}

\item \textbf{True or False:} The logistic regression model learned on $\{\mathbf{X'}, \mathbf{y} \}$ will be equivalent to the model learned on $\{ \mathbf{X}, \mathbf{y} \}$. Here we assume that in both cases we have picked the right gradient descent algorithm to converge to the global optima. The two logistic regression models are equivalent if and only if they predict identical results for all $\mathbf{x}$, $[\mathbf{x}, \mathbf{x}_d]$ pairs, $\mathbf{x} \in \mathbb{R}^d$.

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    True.
    \end{soln}

\item \textbf{True or False:} The duplicated feature will not affect deep neural networks. They are always guaranteed to converge to the \textbf{global} optima as long as we pick the right gradient descent algorithm.

\begin{checkboxes}
     \choice True 
     \choice False
    \end{checkboxes}
    \begin{soln}
    False.
    \end{soln}

\end{enumerate}

\begin{qauthor}
    (1) By Sida, (2) learning objective: not really on feature engineering but testing on various models, "Implement Decision Tree training and prediction", "Implement k-Nearest Neighbors", "closed-form for linear regression", "Distinguish between convex, concave, and nonconvex functions", definition of logistic regression.  and (3) The "one duplicated feature" scenario was in one of the in-class quiz polls when I took 10-701 2017 Fall, and also came up in one interview I had. I fleshed this idea out and tested on all models we have covered.
\end{qauthor}

\question[] \textbf{Select all the true statements:}  Recall in regularization, given an objective function $J(\Theta)$, we have $$\hat{\Theta}=\argmin_{\Theta} J(\Theta) + \lambda r(\Theta)$$
where $r(\Theta)$ is the regularization term.
    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice The performance of the trained classifier on the training set will typically be better than its performance on the test set.
     \choice The recommended way to choose $\lambda$ is to pick the value of $\lambda$ that minimizes the \textbf{training set error}.
     \choice The recommended way to choose $\lambda$ is to pick the value of $\lambda$ that minimizes the \textbf{cross validation error}.
     \choice The recommended way to choose $\lambda$ is to pick the value of $\lambda$ that minimizes the \textbf{test set error}.
    \end{checkboxes}
    }
    \begin{soln}
    A,C\\
    \end{soln}
    \begin{qauthor}
    (1) By: Jeremy Ong (2) Learning Objective: know how to use regularization to combat overfitting (3) From \href{https://github.com/mGalarnyk/datasciencecoursera/blob/master/Stanford_Machine_Learning/Week6/AdviceQuiz.md}{\underline{Coursera Stanford Machine Learning}}
    \end{qauthor}

\end{questions}

\section{Neural Networks}
\begin{questions}
\question Explain a potential problem that may arise while learning a network which uses the activation function illustrated in \ref{fig:relu}.(a), and is avoided if the activation function in \ref{fig:relu}.(b), is used instead
\begin{figure}[!h]
    \centering
    \includegraphics[width = 0.9\textwidth]{fig/relu.png}
    \caption{ReLu and Leaky ReLu activation}
    \label{fig:relu}
\end{figure}
    \fillwithlines{2em}
    \begin{soln}
    If there are many layers to our neural network, the gradient can become zero during training with ReLu as activation function. Therefore the weights will not be updated anymore. 
    \end{soln}
    
    \begin{qauthor}
    (1) Oliver (2) learning objective addressed: gradient descent update, backpropagation, and neural network architecture (3) None
    \end{qauthor}
    
    \question  \textbf{Select all that apply:} Discuss how batch size would affect stochastic gradient descent when training a neural nets:
    
    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice With a bigger batch size, the gradient of a batch will have a larger bias. 
     \choice With a bigger batch size, the gradient of a batch will have a smaller variance. 
     \choice With a smaller batch size, we are guaranteed to converge to a smaller loss value.
     \choice With a smaller batch size, we are likely to converge in fewer epochs
    \end{checkboxes}
    }

    \begin{soln}
    BD
    \end{soln}
    
    \begin{qauthor}
    (1) By Sida (2) learning objective addressed: "stochastic gradient descent with mini-batches" "analyzing the tradeoff of computational complexity vs. convergence speed" (actually the optimization lecture) (3) None
    \end{qauthor}
    
    \question \textbf{Select all that apply:} The XOR function for two binary variables $x$ and $y$ is defined as follows:
\begin{equation*}
    \begin{split}
        f(x,y) & = 0, x = y \\
        & = 1, x \neq y
    \end{split}
\end{equation*}
Based on this information, select all statements which are true:
    {%
    \checkboxchar{$\Box$} % change checkbox style locally
    \begin{checkboxes}
     \choice The XOR function is linearly separable
     \choice A single-layer perceptron is incapable of representing the XOR function
     \choice A multi-layer perceptron is incapable of representing the XOR function
     \choice None of the above
    \end{checkboxes}
    }
    \begin{soln}
    B is the only correct option. The XOR function is not linearly separable, hence a single-layer perceptron cannot represent it but a multi-layer perceptron can.\\
    \end{soln}
    \begin{qauthor}
    (1) Aakanksha (2) Learning objective addressed: Identifying linear separability, Decision boundaries for perceptron and neural networks and (3) no specific source, but this is a well-discussed topic
    \end{qauthor}
    
    \question Backpropagation on a random (non-NN) computation graph.
    
    \begin{qauthor}
    (1) proposed by Sida, someone should flesh this out if time permits. (2) Learning objective addressed: "Carry out the backpropagation on an arbitrary computation graph" (3) None.
    \end{qauthor}
    
\end{questions}

\section{Theory}
\begin{questions}
\question


 \begin{qauthor}
    You're about to get on a plane to Seattle. You want to know  if you should bring an umbrella. You call 3 random friends of yours who live there and ask each independently if it's raining. Each of your friends has a 2/3 chance of telling you the truth and a 1/3 chance of messing with you by lying. All 3 friends tell you that "Yes" it is raining. What is the probability that it's actually raining in Seattle?
    
    (1) Edgar (2) Learning objective addressed: Probability theory, Information Theory & Baye\'s Rule.  Question adapted from interview question I encountered
    \end{qauthor}
\end{questions}

\section{Other}
\begin{questions}
\question
\end{questions}
