\sectionquestion{Pre-Training, fine-tuning, in-context learning}

\begin{parts}

\begin{comment}
    
\part[2] \textbf{Short answer:} Consider a dataset with a large number of unlabelled images and a small set of labelled images. Describe a strategy that combines both supervised and unsupervised pre-training for image classification and explain why this strategy might be more effective than using either method alone.

\fillwithlines{5em}
\begin{soln}
    First apply unsupervised pre-training on the large unlabelled dataset to learn general features and representations of images. Then use supervised pre-training on the smaller labelled dataset to refine the model for specific classification tasks. Used unsupervised learning for capturing broad features without the need for labels, and supervised learning for task-specific fine-tuning, resulting in a more robust and versatile model.
\end{soln}
\begin{qauthor}
    Monica, Pretraining, Fine-tuning \& In-context Learning: Compare and contrast supervised and unsupervised pre-training
    
    Removed by Henry: a bit too open-ended imo
\end{qauthor}
\begin{qtester}
    Probably needs to be broken into parts. More direction on the strategy would also be useful. 
\end{qtester}
\end{comment}

\part[3] \textbf{Order the options:} Suppose you have a large, unlabelled dataset and a small, labelled dataset for some multi-class classification task. Given a fixed neural network architecture with many hidden layers, where the output layer is a softmax layer of the appropriate dimensionality, arrange the steps below in the order of a prototypical \emph{unsupervised} pre-training \& fine-tuning pipeline. 

Specify your answer by writing the letter corresponding to your selection for each of the five steps. \textbf{Note:} if some sequence of steps can be performed in any order, break ties in alphabetical order. 
 
\begin{enumerate}[label=\Alph*]
    \item Replace the output layer of the original neural network architecture with a sigmoid layer. 
    \item Replace the output layer of the original neural network architecture with a linear layer that has the same dimensionality as your input layer. 
    \item Optimize the weights of the entire network simultaneously using the unlabelled dataset by minimizing the reconstruction error. 
    \item Optimize the weights of the entire network simultaneously using the labelled dataset by minimizing the cross-entropy loss. 
    \item Optimize the weights of the network layerwise, starting from the input layer, using the unlabelled dataset by minimizing the reconstruction error.  
    \item Optimize the weights of the network layerwise, starting from the input layer, using the labelled dataset by minimizing the cross-entropy loss. 
    \item Initialize the weights of your network randomly.
    \item Initialize the weights of your network to the pre-trained weights. 
\end{enumerate}
\begin{comment}
\mbox{\begin{tcolorbox}[enhanced, sidebyside align=top, box align=top, height=2.5cm, width=3cm, colback=white, borderline={1pt}{-2pt} colbacktitle=black, title=Step 1]
        %solution
\end{tcolorbox}\hspace{1em}\begin{tcolorbox}[enhanced, sidebyside align=top, box align=top, height=2.5cm, width=3cm, colback=white, borderline={1pt}{-2pt} colbacktitle=black, title=Step 2]
        %solution
\end{tcolorbox}\hspace{1em}\begin{tcolorbox}[enhanced, sidebyside align=top, box align=top, height=2.5cm, width=3cm, colback=white, borderline={1pt}{-2pt} colbacktitle=black, title=Step 3]
        %solution
\end{tcolorbox}\hspace{1em}\begin{tcolorbox}[enhanced, sidebyside align=top, box align=top, height=2.5cm, width=3cm, colback=white, borderline={1pt}{-2pt} colbacktitle=black, title=Step 4]
        %solution
\end{tcolorbox}\hspace{1em}\begin{tcolorbox}[enhanced, sidebyside align=top, box align=top, height=2.5cm, width=3cm, colback=white, borderline={1pt}{-2pt} colbacktitle=black, title=Step 5]
        %solution
\end{tcolorbox}}
\end{comment}
\mbox{\begin{your_solution}[height=2.5cm, width=3cm, title=Step 1]
        %solution
\end{your_solution}\hspace{0.5em}\begin{your_solution}[height=2.5cm, width=3cm, title=Step 2]
        %solution
\end{your_solution}\hspace{0.5em}\begin{your_solution}[height=2.5cm, width=3cm, title=Step 3]
        %solution
\end{your_solution}\hspace{0.5em}\begin{your_solution}[height=2.5cm, width=3cm, title=Step 4]
        %solution
\end{your_solution}\hspace{0.5em}\begin{your_solution}[height=2.5cm, width=3cm, title=Step 5]
        %solution
\end{your_solution}}

\begin{soln}
    B - G - E - H - D
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\clearpage

\part[1] \textbf{Select one:} Which of the following options best describes the role of human feedback in Reinforcement Learning from Human Feedback (RLHF)?
\begin{checkboxes}
    \choice Human feedback is used to pre-train a model on general tasks before any specific training.
    \choice Human feedback is used to train a reward function for reinforcement learning. 
    \choice Human feedback is used to tune model hyperparameters.
    \choice Human feedback is used to evaluate a model after training has convereged. 
\end{checkboxes}
    
\begin{soln}
    B: RLHF uses feedback from humans to guide the training process, making it more nuanced and aligned with human values and judgments.
\end{soln}
\begin{qauthor}
    Monica, Pretraining, Fine-tuning \& In-context Learning: Define reinforcement learning from human feedback (RLHF) 

    Lightly edited by Henry
\end{qauthor}
\begin{qtester}
    LGTM
\end{qtester}

\part[2] \textbf{Select all that apply:} Given a pre-trained language model, which of the following procedures directly change the pre-trained model's weights? 
{%
    \checkboxchar{$\Box$} \checkedchar{$\blacksquare$} % change checkbox style locally
    \begin{checkboxes}
        \choice In-context Learning
        \choice Supervised Fine-tuning
        \choice Zero-shot Learning
        \choice Reinforcement Learning from Human Feedback
        \choice None of the above
    \end{checkboxes}
}
\begin{soln}
    B and D; In-context learning refers to the LLMs ability to learn from the inputs without updating any model parameters. Zero-shot learning is a form of in-context learning. 
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\part[2] \textbf{Short answer:} Given a pre-trained \underline{large} language model (LLM) and some amount of labelled training data, what is one reason why it might be better to perform in-context learning using the LLM rather than  fine-tune the LLM using the training dataset? 
\fillwithlines{9em}
\begin{soln}
    You don't have enough labelled data to effectively fine-tune the parameters \emph{AND} you don't have enough computational resources to perform supervised fine-tuning. 
\end{soln}
\begin{qauthor}
    Henry
\end{qauthor}

\begin{comment}
\part[2] \textbf{Short answer:}Neural The Narwhal is tasked with developing a language model for the diamond market at a startup. Faced with limited data availability, Tensor the Tiger advises improving the model's performance by fine-tuning. Which approach should Neural consider to best address this challenge?
\fillwithlines{10em}
\begin{soln}
\textbf{Solution}: Since fine-tuning typically requires a substantial amount of specific, labeled data, which is not available in this case, Neural should leverage in-context learning. This approach involves providing the language model with carefully crafted prompts and examples related to the diamond market.\\
\end{soln}
\begin{qauthor}
    Sahithya, Provide examples where in-context learning might be necessary/preferable relative to supervised fine-tuning.
    
    Removed by Henry due to ambiguity
\end{qauthor}
\begin{qtester}
        Question needs to specify what approaches the answer can be selected out of. Right now the question says Neural is going to fine-tune but Neural doesn't actually fine-tune.    
\end{qtester}
\end{comment}

\end{parts}